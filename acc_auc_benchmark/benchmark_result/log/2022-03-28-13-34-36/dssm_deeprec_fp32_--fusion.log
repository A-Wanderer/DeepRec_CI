INFO:tensorflow:Parsing ./data/taobao_train_data
INFO:tensorflow:Parsing ./data/taobao_test_data
INFO:tensorflow:Is using fused embedding lookup for this scope age_level_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope cms_group_id_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope cms_segid_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope new_user_class_level_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope occupation_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope pvalue_level_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope shopping_level_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope tag_brand_list_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope tag_category_list_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope user_id_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope adgroup_id_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope brand_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope campaign_id_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope cate_id_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope customer_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope pid_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope price_embedding_weights
2022-03-28 07:42:16.096897: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3400000000 Hz
2022-03-28 07:42:16.099843: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3ec6bf0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-03-28 07:42:16.099895: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
Using TensorFlow version 1.15.5
Checking dataset
Numbers of training dataset is 50000
Numbers of test dataset is 10000
Saving model checkpoints to /benchmark_result/checkpoint/2022-03-28-13-34-36/dssm_deeprec_fp32_--fusion
global_step/sec: 33.2540
loss = 1912.866943359375, steps = 0, cost time = 3.01s
loss = 1912.866943359375, steps = 0
global_step/sec: 28.3914
loss = 1285.843505859375, steps = 100, cost time = 3.52s
loss = 1285.843505859375, steps = 100
global_step/sec: 26.1772
loss = 1113.1204833984375, steps = 200, cost time = 3.82s
loss = 1113.1204833984375, steps = 200
global_step/sec: 29.9224
loss = 1210.836669921875, steps = 300, cost time = 3.34s
loss = 1210.836669921875, steps = 300
global_step/sec: 28.4433
loss = 987.6087646484375, steps = 400, cost time = 3.52s
loss = 987.6087646484375, steps = 400
global_step/sec: 28.7383
loss = 1263.285888671875, steps = 500, cost time = 3.48s
loss = 1263.285888671875, steps = 500
global_step/sec: 27.6522
loss = 1114.930908203125, steps = 600, cost time = 3.62s
loss = 1114.930908203125, steps = 600
global_step/sec: 27.6898
loss = 963.5902709960938, steps = 700, cost time = 3.61s
loss = 963.5902709960938, steps = 700
global_step/sec: 27.8405
loss = 1183.331787109375, steps = 800, cost time = 3.59s
loss = 1183.331787109375, steps = 800
global_step/sec: 28.4426
loss = 1189.215576171875, steps = 900, cost time = 3.52s
loss = 1189.215576171875, steps = 900
global_step/sec: 28.5565
loss = 1099.8726806640625, steps = 1000, cost time = 3.50s
loss = 1099.8726806640625, steps = 1000
global_step/sec: 27.9649
loss = 1195.015625, steps = 1100, cost time = 3.58s
loss = 1195.015625, steps = 1100
global_step/sec: 27.5708
loss = 1259.02099609375, steps = 1200, cost time = 3.63s
loss = 1259.02099609375, steps = 1200
global_step/sec: 28.0554
loss = 1092.865478515625, steps = 1300, cost time = 3.56s
loss = 1092.865478515625, steps = 1300
global_step/sec: 27.9323
loss = 985.968017578125, steps = 1400, cost time = 3.58s
loss = 985.968017578125, steps = 1400
global_step/sec: 28.0624
loss = 1220.022705078125, steps = 1500, cost time = 3.56s
loss = 1220.022705078125, steps = 1500
global_step/sec: 27.9687
loss = 1245.608154296875, steps = 1600, cost time = 3.58s
loss = 1245.608154296875, steps = 1600
global_step/sec: 27.5526
loss = 1175.0294189453125, steps = 1700, cost time = 3.63s
loss = 1175.0294189453125, steps = 1700
global_step/sec: 27.7689
loss = 1010.583251953125, steps = 1800, cost time = 3.60s
loss = 1010.583251953125, steps = 1800
global_step/sec: 28.4138
loss = 1277.617431640625, steps = 1900, cost time = 3.52s
loss = 1277.617431640625, steps = 1900
global_step/sec: 27.8346
loss = 1155.7003173828125, steps = 2000, cost time = 3.59s
loss = 1155.7003173828125, steps = 2000
global_step/sec: 27.7266
loss = 1322.7926025390625, steps = 2100, cost time = 3.61s
loss = 1322.7926025390625, steps = 2100
global_step/sec: 28.3188
loss = 1104.7880859375, steps = 2200, cost time = 3.53s
loss = 1104.7880859375, steps = 2200
global_step/sec: 27.8674
loss = 1232.348876953125, steps = 2300, cost time = 3.59s
loss = 1232.348876953125, steps = 2300
global_step/sec: 27.9295
loss = 1258.038818359375, steps = 2400, cost time = 3.58s
loss = 1258.038818359375, steps = 2400
global_step/sec: 27.9983
loss = 1004.0253295898438, steps = 2500, cost time = 3.57s
loss = 1004.0253295898438, steps = 2500
global_step/sec: 27.9371
loss = 1168.215087890625, steps = 2600, cost time = 3.58s
loss = 1168.215087890625, steps = 2600
global_step/sec: 27.8274
loss = 1168.18212890625, steps = 2700, cost time = 3.59s
loss = 1168.18212890625, steps = 2700
global_step/sec: 28.2209
loss = 1187.33984375, steps = 2800, cost time = 3.54s
loss = 1187.33984375, steps = 2800
global_step/sec: 28.0297
loss = 1129.0205078125, steps = 2900, cost time = 3.57s
loss = 1129.0205078125, steps = 2900
global_step/sec: 27.9988
loss = 1123.881591796875, steps = 3000, cost time = 3.57s
loss = 1123.881591796875, steps = 3000
global_step/sec: 28.0411
loss = 1130.0146484375, steps = 3100, cost time = 3.57s
loss = 1130.0146484375, steps = 3100
global_step/sec: 28.2691
loss = 1117.2628173828125, steps = 3200, cost time = 3.54s
loss = 1117.2628173828125, steps = 3200
global_step/sec: 28.2009
loss = 1117.232666015625, steps = 3300, cost time = 3.55s
loss = 1117.232666015625, steps = 3300
global_step/sec: 27.7403
loss = 1296.6021728515625, steps = 3400, cost time = 3.60s
loss = 1296.6021728515625, steps = 3400
global_step/sec: 28.0046
loss = 1187.2347412109375, steps = 3500, cost time = 3.57s
loss = 1187.2347412109375, steps = 3500
global_step/sec: 28.4246
loss = 1174.4407958984375, steps = 3600, cost time = 3.52s
loss = 1174.4407958984375, steps = 3600
global_step/sec: 28.3175
loss = 1174.4144287109375, steps = 3700, cost time = 3.53s
loss = 1174.4144287109375, steps = 3700
global_step/sec: 28.3223
loss = 978.8051147460938, steps = 3800, cost time = 3.53s
loss = 978.8051147460938, steps = 3800
global_step/sec: 27.6929
loss = 1168.0008544921875, steps = 3900, cost time = 3.61s
loss = 1168.0008544921875, steps = 3900
global_step/sec: 28.7184
loss = 1244.8873291015625, steps = 4000, cost time = 3.48s
loss = 1244.8873291015625, steps = 4000
global_step/sec: 28.0277
loss = 1053.9127197265625, steps = 4100, cost time = 3.57s
loss = 1053.9127197265625, steps = 4100
global_step/sec: 28.1262
loss = 1066.498291015625, steps = 4200, cost time = 3.56s
loss = 1066.498291015625, steps = 4200
global_step/sec: 28.4128
loss = 966.2953491210938, steps = 4300, cost time = 3.52s
loss = 966.2953491210938, steps = 4300
global_step/sec: 28.7570
loss = 1322.343994140625, steps = 4400, cost time = 3.48s
loss = 1322.343994140625, steps = 4400
global_step/sec: 27.9027
loss = 1328.817626953125, steps = 4500, cost time = 3.58s
loss = 1328.817626953125, steps = 4500
global_step/sec: 28.1132
loss = 1142.4576416015625, steps = 4600, cost time = 3.56s
loss = 1142.4576416015625, steps = 4600
global_step/sec: 27.8512
loss = 1110.69287109375, steps = 4700, cost time = 3.59s
loss = 1110.69287109375, steps = 4700
global_step/sec: 27.3559
loss = 1072.74755859375, steps = 4800, cost time = 3.66s
loss = 1072.74755859375, steps = 4800
global_step/sec: 27.8183
loss = 1167.904296875, steps = 4900, cost time = 3.59s
loss = 1167.904296875, steps = 4900
global_step/sec: 27.9754
loss = 972.4537963867188, steps = 5000, cost time = 3.57s
loss = 972.4537963867188, steps = 5000
global_step/sec: 27.6678
loss = 1129.696533203125, steps = 5100, cost time = 3.61s
loss = 1129.696533203125, steps = 5100
global_step/sec: 27.7540
loss = 1148.76171875, steps = 5200, cost time = 3.60s
loss = 1148.76171875, steps = 5200
global_step/sec: 27.6121
loss = 1180.650390625, steps = 5300, cost time = 3.62s
loss = 1180.650390625, steps = 5300
global_step/sec: 27.3529
loss = 1219.055908203125, steps = 5400, cost time = 3.66s
loss = 1219.055908203125, steps = 5400
global_step/sec: 27.8495
loss = 1315.7779541015625, steps = 5500, cost time = 3.59s
loss = 1315.7779541015625, steps = 5500
global_step/sec: 27.7740
loss = 1219.0499267578125, steps = 5600, cost time = 3.60s
loss = 1219.0499267578125, steps = 5600
global_step/sec: 27.9735
loss = 1028.6575927734375, steps = 5700, cost time = 3.57s
loss = 1028.6575927734375, steps = 5700
global_step/sec: 28.3915
loss = 1187.0101318359375, steps = 5800, cost time = 3.52s
loss = 1187.0101318359375, steps = 5800
global_step/sec: 27.3751
loss = 1136.00830078125, steps = 5900, cost time = 3.65s
loss = 1136.00830078125, steps = 5900
global_step/sec: 28.0606
loss = 1142.3646240234375, steps = 6000, cost time = 3.56s
loss = 1142.3646240234375, steps = 6000
global_step/sec: 28.0288
loss = 1199.7965087890625, steps = 6100, cost time = 3.57s
loss = 1199.7965087890625, steps = 6100
global_step/sec: 27.9429
loss = 1110.609619140625, steps = 6200, cost time = 3.58s
loss = 1110.609619140625, steps = 6200
global_step/sec: 27.9049
loss = 1009.8364868164062, steps = 6300, cost time = 3.58s
loss = 1009.8364868164062, steps = 6300
global_step/sec: 28.1021
loss = 1104.2642822265625, steps = 6400, cost time = 3.56s
loss = 1104.2642822265625, steps = 6400
global_step/sec: 27.8360
loss = 1097.929443359375, steps = 6500, cost time = 3.59s
loss = 1097.929443359375, steps = 6500
global_step/sec: 28.5008
loss = 1186.982421875, steps = 6600, cost time = 3.51s
loss = 1186.982421875, steps = 6600
global_step/sec: 27.9041
loss = 1283.3837890625, steps = 6700, cost time = 3.58s
loss = 1283.3837890625, steps = 6700
global_step/sec: 28.0477
loss = 1167.81298828125, steps = 6800, cost time = 3.57s
loss = 1167.81298828125, steps = 6800
global_step/sec: 27.5229
loss = 1167.8125, steps = 6900, cost time = 3.63s
loss = 1167.8125, steps = 6900
global_step/sec: 28.1227
loss = 1212.5760498046875, steps = 7000, cost time = 3.56s
loss = 1212.5760498046875, steps = 7000
global_step/sec: 28.1202
loss = 1199.7623291015625, steps = 7100, cost time = 3.56s
loss = 1199.7623291015625, steps = 7100
global_step/sec: 27.9287
loss = 1129.60693359375, steps = 7200, cost time = 3.58s
loss = 1129.60693359375, steps = 7200
global_step/sec: 27.9783
loss = 1091.579833984375, steps = 7300, cost time = 3.57s
loss = 1091.579833984375, steps = 7300
global_step/sec: 28.3148
loss = 1129.6016845703125, steps = 7400, cost time = 3.53s
loss = 1129.6016845703125, steps = 7400
global_step/sec: 27.7402
loss = 1016.0603637695312, steps = 7500, cost time = 3.60s
loss = 1016.0603637695312, steps = 7500
global_step/sec: 27.8632
loss = 1135.94921875, steps = 7600, cost time = 3.59s
loss = 1135.94921875, steps = 7600
global_step/sec: 27.8473
loss = 1193.350830078125, steps = 7700, cost time = 3.59s
loss = 1193.350830078125, steps = 7700
global_step/sec: 28.1479
loss = 1167.7886962890625, steps = 7800, cost time = 3.55s
loss = 1167.7886962890625, steps = 7800
global_step/sec: 27.7668
loss = 1085.2430419921875, steps = 7900, cost time = 3.60s
loss = 1085.2430419921875, steps = 7900
global_step/sec: 27.9567
loss = 1193.34228515625, steps = 8000, cost time = 3.58s
loss = 1193.34228515625, steps = 8000
global_step/sec: 28.0794
loss = 1041.1431884765625, steps = 8100, cost time = 3.56s
loss = 1041.1431884765625, steps = 8100
global_step/sec: 27.8123
loss = 1206.142333984375, steps = 8200, cost time = 3.60s
loss = 1206.142333984375, steps = 8200
global_step/sec: 28.8486
loss = 1354.65234375, steps = 8300, cost time = 3.47s
loss = 1354.65234375, steps = 8300
global_step/sec: 28.4209
loss = 997.2713623046875, steps = 8400, cost time = 3.52s
loss = 997.2713623046875, steps = 8400
global_step/sec: 28.1108
loss = 1135.942626953125, steps = 8500, cost time = 3.56s
loss = 1135.942626953125, steps = 8500
global_step/sec: 27.9239
loss = 1174.154052734375, steps = 8600, cost time = 3.58s
loss = 1174.154052734375, steps = 8600
global_step/sec: 27.9967
loss = 1218.9581298828125, steps = 8700, cost time = 3.57s
loss = 1218.9581298828125, steps = 8700
global_step/sec: 28.3825
loss = 1218.9619140625, steps = 8800, cost time = 3.52s
loss = 1218.9619140625, steps = 8800
global_step/sec: 28.0164
loss = 1110.5379638671875, steps = 8900, cost time = 3.57s
loss = 1110.5379638671875, steps = 8900
global_step/sec: 27.9809
loss = 959.8834838867188, steps = 9000, cost time = 3.57s
loss = 959.8834838867188, steps = 9000
global_step/sec: 27.9353
loss = 1206.127197265625, steps = 9100, cost time = 3.58s
loss = 1206.127197265625, steps = 9100
global_step/sec: 27.8350
loss = 1296.255859375, steps = 9200, cost time = 3.59s
loss = 1296.255859375, steps = 9200
global_step/sec: 28.4621
loss = 1091.541259765625, steps = 9300, cost time = 3.51s
loss = 1091.541259765625, steps = 9300
global_step/sec: 27.8113
loss = 1167.757568359375, steps = 9400, cost time = 3.60s
loss = 1167.757568359375, steps = 9400
global_step/sec: 27.8393
loss = 1276.864501953125, steps = 9500, cost time = 3.59s
loss = 1276.864501953125, steps = 9500
global_step/sec: 27.8127
loss = 1257.52001953125, steps = 9600, cost time = 3.60s
loss = 1257.52001953125, steps = 9600
global_step/sec: 28.2014
loss = 1296.2490234375, steps = 9700, cost time = 3.55s
loss = 1296.2490234375, steps = 9700
global_step/sec: 28.2201
loss = 1072.58837890625, steps = 9800, cost time = 3.54s
loss = 1072.58837890625, steps = 9800
global_step/sec: 27.9722
loss = 1110.521240234375, steps = 9900, cost time = 3.57s
loss = 1110.521240234375, steps = 9900
global_step/sec: 28.0983
loss = 1135.9114990234375, steps = 10000, cost time = 3.56s
loss = 1135.9114990234375, steps = 10000
global_step/sec: 27.8047
loss = 1432.985107421875, steps = 10100, cost time = 3.60s
loss = 1432.985107421875, steps = 10100
global_step/sec: 27.9639
loss = 1123.2044677734375, steps = 10200, cost time = 3.58s
loss = 1123.2044677734375, steps = 10200
global_step/sec: 27.7963
loss = 1104.18701171875, steps = 10300, cost time = 3.60s
loss = 1104.18701171875, steps = 10300
global_step/sec: 27.7524
loss = 1212.523681640625, steps = 10400, cost time = 3.60s
loss = 1212.523681640625, steps = 10400
global_step/sec: 28.0801
loss = 1034.8243408203125, steps = 10500, cost time = 3.56s
loss = 1034.8243408203125, steps = 10500
global_step/sec: 27.8691
loss = 1148.628173828125, steps = 10600, cost time = 3.59s
loss = 1148.628173828125, steps = 10600
global_step/sec: 28.1754
loss = 1110.5205078125, steps = 10700, cost time = 3.55s
loss = 1110.5205078125, steps = 10700
global_step/sec: 28.3314
loss = 1059.974365234375, steps = 10800, cost time = 3.53s
loss = 1059.974365234375, steps = 10800
global_step/sec: 28.3705
loss = 1186.90625, steps = 10900, cost time = 3.52s
loss = 1186.90625, steps = 10900
global_step/sec: 28.1530
loss = 1097.846923828125, steps = 11000, cost time = 3.55s
loss = 1097.846923828125, steps = 11000
global_step/sec: 27.8685
loss = 1034.815673828125, steps = 11100, cost time = 3.59s
loss = 1034.815673828125, steps = 11100
global_step/sec: 27.6744
loss = 1148.6224365234375, steps = 11200, cost time = 3.61s
loss = 1148.6224365234375, steps = 11200
global_step/sec: 27.8159
loss = 1078.8848876953125, steps = 11300, cost time = 3.60s
loss = 1078.8848876953125, steps = 11300
global_step/sec: 28.1386
loss = 1148.6201171875, steps = 11400, cost time = 3.55s
loss = 1148.6201171875, steps = 11400
global_step/sec: 27.7489
loss = 1180.5057373046875, steps = 11500, cost time = 3.60s
loss = 1180.5057373046875, steps = 11500
global_step/sec: 28.0421
loss = 1193.296142578125, steps = 11600, cost time = 3.57s
loss = 1193.296142578125, steps = 11600
global_step/sec: 28.1896
loss = 1116.85400390625, steps = 11700, cost time = 3.55s
loss = 1116.85400390625, steps = 11700
global_step/sec: 27.9730
loss = 1167.73974609375, steps = 11800, cost time = 3.57s
loss = 1167.73974609375, steps = 11800
global_step/sec: 28.1792
loss = 1154.988525390625, steps = 11900, cost time = 3.55s
loss = 1154.988525390625, steps = 11900
global_step/sec: 28.0019
loss = 1174.1163330078125, steps = 12000, cost time = 3.57s
loss = 1174.1163330078125, steps = 12000
global_step/sec: 27.9147
loss = 1174.117431640625, steps = 12100, cost time = 3.58s
loss = 1174.117431640625, steps = 12100
global_step/sec: 27.7173
loss = 1193.2928466796875, steps = 12200, cost time = 3.61s
loss = 1193.2928466796875, steps = 12200
global_step/sec: 64391.6702
loss = 23.200855255126953, steps = 12207, cost time = 0.19s
loss = 23.200855255126953, steps = 12207
Evaluation complate:[3/3]
ACC = 0.9466999769210815
AUC = 0.5053579807281494
