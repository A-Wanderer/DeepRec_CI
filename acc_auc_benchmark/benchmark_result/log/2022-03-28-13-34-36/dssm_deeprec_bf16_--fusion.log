INFO:tensorflow:Parsing ./data/taobao_train_data
INFO:tensorflow:Parsing ./data/taobao_test_data
INFO:tensorflow:Is using fused embedding lookup for this scope age_level_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope cms_group_id_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope cms_segid_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope new_user_class_level_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope occupation_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope pvalue_level_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope shopping_level_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope tag_brand_list_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope tag_category_list_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope user_id_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope adgroup_id_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope brand_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope campaign_id_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope cate_id_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope customer_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope pid_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope price_embedding_weights
2022-03-28 07:58:56.012580: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3400000000 Hz
2022-03-28 07:58:56.016094: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3f66ea0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-03-28 07:58:56.016141: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
Using TensorFlow version 1.15.5
Checking dataset
Numbers of training dataset is 50000
Numbers of test dataset is 10000
Saving model checkpoints to /benchmark_result/checkpoint/2022-03-28-13-34-36/dssm_deeprec_bf16_--fusion
global_step/sec: 31.2164
loss = 1912.870361328125, steps = 0, cost time = 3.20s
loss = 1912.870361328125, steps = 0
global_step/sec: 25.0000
loss = 1280.1822509765625, steps = 100, cost time = 4.00s
loss = 1280.1822509765625, steps = 100
global_step/sec: 23.1070
loss = 1122.3934326171875, steps = 200, cost time = 4.33s
loss = 1122.3934326171875, steps = 200
global_step/sec: 27.5539
loss = 1211.560546875, steps = 300, cost time = 3.63s
loss = 1211.560546875, steps = 300
global_step/sec: 25.2361
loss = 987.2120971679688, steps = 400, cost time = 3.96s
loss = 987.2120971679688, steps = 400
global_step/sec: 25.5590
loss = 1263.2265625, steps = 500, cost time = 3.91s
loss = 1263.2265625, steps = 500
global_step/sec: 25.0244
loss = 1114.849609375, steps = 600, cost time = 4.00s
loss = 1114.849609375, steps = 600
global_step/sec: 25.0489
loss = 963.2774047851562, steps = 700, cost time = 3.99s
loss = 963.2774047851562, steps = 700
global_step/sec: 25.2938
loss = 1183.34814453125, steps = 800, cost time = 3.95s
loss = 1183.34814453125, steps = 800
global_step/sec: 24.4336
loss = 1189.228515625, steps = 900, cost time = 4.09s
loss = 1189.228515625, steps = 900
global_step/sec: 25.0332
loss = 1099.874755859375, steps = 1000, cost time = 3.99s
loss = 1099.874755859375, steps = 1000
global_step/sec: 24.8242
loss = 1195.013671875, steps = 1100, cost time = 4.03s
loss = 1195.013671875, steps = 1100
global_step/sec: 24.6633
loss = 1259.00830078125, steps = 1200, cost time = 4.05s
loss = 1259.00830078125, steps = 1200
global_step/sec: 24.7228
loss = 1092.87109375, steps = 1300, cost time = 4.04s
loss = 1092.87109375, steps = 1300
global_step/sec: 24.7057
loss = 985.951171875, steps = 1400, cost time = 4.05s
loss = 985.951171875, steps = 1400
global_step/sec: 25.2086
loss = 1220.001953125, steps = 1500, cost time = 3.97s
loss = 1220.001953125, steps = 1500
global_step/sec: 25.0478
loss = 1245.6046142578125, steps = 1600, cost time = 3.99s
loss = 1245.6046142578125, steps = 1600
global_step/sec: 24.9294
loss = 1175.025146484375, steps = 1700, cost time = 4.01s
loss = 1175.025146484375, steps = 1700
global_step/sec: 25.0425
loss = 1010.6503295898438, steps = 1800, cost time = 3.99s
loss = 1010.6503295898438, steps = 1800
global_step/sec: 25.1516
loss = 1277.727783203125, steps = 1900, cost time = 3.98s
loss = 1277.727783203125, steps = 1900
global_step/sec: 25.2238
loss = 1155.717041015625, steps = 2000, cost time = 3.96s
loss = 1155.717041015625, steps = 2000
global_step/sec: 25.1595
loss = 1322.7989501953125, steps = 2100, cost time = 3.97s
loss = 1322.7989501953125, steps = 2100
global_step/sec: 24.8665
loss = 1104.7874755859375, steps = 2200, cost time = 4.02s
loss = 1104.7874755859375, steps = 2200
global_step/sec: 25.3769
loss = 1232.335205078125, steps = 2300, cost time = 3.94s
loss = 1232.335205078125, steps = 2300
global_step/sec: 24.8799
loss = 1258.030517578125, steps = 2400, cost time = 4.02s
loss = 1258.030517578125, steps = 2400
global_step/sec: 25.0930
loss = 1004.01123046875, steps = 2500, cost time = 3.99s
loss = 1004.01123046875, steps = 2500
global_step/sec: 24.6852
loss = 1168.205078125, steps = 2600, cost time = 4.05s
loss = 1168.205078125, steps = 2600
global_step/sec: 24.9648
loss = 1168.1767578125, steps = 2700, cost time = 4.01s
loss = 1168.1767578125, steps = 2700
global_step/sec: 24.8234
loss = 1187.3162841796875, steps = 2800, cost time = 4.03s
loss = 1187.3162841796875, steps = 2800
global_step/sec: 25.1896
loss = 1123.5880126953125, steps = 2900, cost time = 3.97s
loss = 1123.5880126953125, steps = 2900
global_step/sec: 24.7445
loss = 1123.56494140625, steps = 3000, cost time = 4.04s
loss = 1123.56494140625, steps = 3000
global_step/sec: 24.9803
loss = 1129.892822265625, steps = 3100, cost time = 4.00s
loss = 1129.892822265625, steps = 3100
global_step/sec: 25.0810
loss = 1117.17822265625, steps = 3200, cost time = 3.99s
loss = 1117.17822265625, steps = 3200
global_step/sec: 25.1448
loss = 1117.1640625, steps = 3300, cost time = 3.98s
loss = 1117.1640625, steps = 3300
global_step/sec: 24.9955
loss = 1296.532470703125, steps = 3400, cost time = 4.00s
loss = 1296.532470703125, steps = 3400
global_step/sec: 24.8654
loss = 1187.1859130859375, steps = 3500, cost time = 4.02s
loss = 1187.1859130859375, steps = 3500
global_step/sec: 25.0219
loss = 1174.3922119140625, steps = 3600, cost time = 4.00s
loss = 1174.3922119140625, steps = 3600
global_step/sec: 25.1091
loss = 1174.376953125, steps = 3700, cost time = 3.98s
loss = 1174.376953125, steps = 3700
global_step/sec: 24.3966
loss = 978.7684326171875, steps = 3800, cost time = 4.10s
loss = 978.7684326171875, steps = 3800
global_step/sec: 25.1789
loss = 1167.96875, steps = 3900, cost time = 3.97s
loss = 1167.96875, steps = 3900
global_step/sec: 25.2239
loss = 1244.845458984375, steps = 4000, cost time = 3.96s
loss = 1244.845458984375, steps = 4000
global_step/sec: 25.1316
loss = 1053.890869140625, steps = 4100, cost time = 3.98s
loss = 1053.890869140625, steps = 4100
global_step/sec: 24.8022
loss = 1066.4716796875, steps = 4200, cost time = 4.03s
loss = 1066.4716796875, steps = 4200
global_step/sec: 25.3774
loss = 966.2626342773438, steps = 4300, cost time = 3.94s
loss = 966.2626342773438, steps = 4300
global_step/sec: 25.2157
loss = 1322.326904296875, steps = 4400, cost time = 3.97s
loss = 1322.326904296875, steps = 4400
global_step/sec: 25.1480
loss = 1328.7958984375, steps = 4500, cost time = 3.98s
loss = 1328.7958984375, steps = 4500
global_step/sec: 25.4667
loss = 1142.434814453125, steps = 4600, cost time = 3.93s
loss = 1142.434814453125, steps = 4600
global_step/sec: 25.1462
loss = 1110.6700439453125, steps = 4700, cost time = 3.98s
loss = 1110.6700439453125, steps = 4700
global_step/sec: 24.9561
loss = 1072.7264404296875, steps = 4800, cost time = 4.01s
loss = 1072.7264404296875, steps = 4800
global_step/sec: 25.3273
loss = 1167.8853759765625, steps = 4900, cost time = 3.95s
loss = 1167.8853759765625, steps = 4900
global_step/sec: 24.8874
loss = 972.4381103515625, steps = 5000, cost time = 4.02s
loss = 972.4381103515625, steps = 5000
global_step/sec: 24.9224
loss = 1129.6806640625, steps = 5100, cost time = 4.01s
loss = 1129.6806640625, steps = 5100
global_step/sec: 25.0081
loss = 1148.747802734375, steps = 5200, cost time = 4.00s
loss = 1148.747802734375, steps = 5200
global_step/sec: 25.3566
loss = 1180.63525390625, steps = 5300, cost time = 3.94s
loss = 1180.63525390625, steps = 5300
global_step/sec: 25.3164
loss = 1219.04248046875, steps = 5400, cost time = 3.95s
loss = 1219.04248046875, steps = 5400
global_step/sec: 25.2382
loss = 1315.76513671875, steps = 5500, cost time = 3.96s
loss = 1315.76513671875, steps = 5500
global_step/sec: 24.9275
loss = 1219.03564453125, steps = 5600, cost time = 4.01s
loss = 1219.03564453125, steps = 5600
global_step/sec: 25.7267
loss = 1028.6448974609375, steps = 5700, cost time = 3.89s
loss = 1028.6448974609375, steps = 5700
global_step/sec: 25.4692
loss = 1186.9989013671875, steps = 5800, cost time = 3.93s
loss = 1186.9989013671875, steps = 5800
global_step/sec: 24.9565
loss = 1135.9931640625, steps = 5900, cost time = 4.01s
loss = 1135.9931640625, steps = 5900
global_step/sec: 25.1650
loss = 1142.34765625, steps = 6000, cost time = 3.97s
loss = 1142.34765625, steps = 6000
global_step/sec: 25.4954
loss = 1199.793212890625, steps = 6100, cost time = 3.92s
loss = 1199.793212890625, steps = 6100
global_step/sec: 25.1195
loss = 1110.5985107421875, steps = 6200, cost time = 3.98s
loss = 1110.5985107421875, steps = 6200
global_step/sec: 25.6191
loss = 1009.8214111328125, steps = 6300, cost time = 3.90s
loss = 1009.8214111328125, steps = 6300
global_step/sec: 25.2476
loss = 1104.254150390625, steps = 6400, cost time = 3.96s
loss = 1104.254150390625, steps = 6400
global_step/sec: 24.4157
loss = 1097.9190673828125, steps = 6500, cost time = 4.10s
loss = 1097.9190673828125, steps = 6500
global_step/sec: 24.9409
loss = 1186.9703369140625, steps = 6600, cost time = 4.01s
loss = 1186.9703369140625, steps = 6600
global_step/sec: 25.5656
loss = 1283.37255859375, steps = 6700, cost time = 3.91s
loss = 1283.37255859375, steps = 6700
global_step/sec: 24.6228
loss = 1167.80126953125, steps = 6800, cost time = 4.06s
loss = 1167.80126953125, steps = 6800
global_step/sec: 24.7280
loss = 1167.800048828125, steps = 6900, cost time = 4.04s
loss = 1167.800048828125, steps = 6900
global_step/sec: 25.0418
loss = 1212.5657958984375, steps = 7000, cost time = 3.99s
loss = 1212.5657958984375, steps = 7000
global_step/sec: 25.3307
loss = 1199.75146484375, steps = 7100, cost time = 3.95s
loss = 1199.75146484375, steps = 7100
global_step/sec: 25.0442
loss = 1129.5955810546875, steps = 7200, cost time = 3.99s
loss = 1129.5955810546875, steps = 7200
global_step/sec: 25.0796
loss = 1091.5693359375, steps = 7300, cost time = 3.99s
loss = 1091.5693359375, steps = 7300
global_step/sec: 24.6722
loss = 1129.5880126953125, steps = 7400, cost time = 4.05s
loss = 1129.5880126953125, steps = 7400
global_step/sec: 24.7588
loss = 1016.052490234375, steps = 7500, cost time = 4.04s
loss = 1016.052490234375, steps = 7500
global_step/sec: 24.9077
loss = 1135.94482421875, steps = 7600, cost time = 4.01s
loss = 1135.94482421875, steps = 7600
global_step/sec: 25.2998
loss = 1193.338134765625, steps = 7700, cost time = 3.95s
loss = 1193.338134765625, steps = 7700
global_step/sec: 25.1146
loss = 1167.7794189453125, steps = 7800, cost time = 3.98s
loss = 1167.7794189453125, steps = 7800
global_step/sec: 25.0983
loss = 1085.236328125, steps = 7900, cost time = 3.98s
loss = 1085.236328125, steps = 7900
global_step/sec: 24.8753
loss = 1193.332763671875, steps = 8000, cost time = 4.02s
loss = 1193.332763671875, steps = 8000
global_step/sec: 25.4348
loss = 1041.126708984375, steps = 8100, cost time = 3.93s
loss = 1041.126708984375, steps = 8100
global_step/sec: 24.9868
loss = 1206.1378173828125, steps = 8200, cost time = 4.00s
loss = 1206.1378173828125, steps = 8200
global_step/sec: 25.1720
loss = 1354.64599609375, steps = 8300, cost time = 3.97s
loss = 1354.64599609375, steps = 8300
global_step/sec: 25.0285
loss = 997.2635498046875, steps = 8400, cost time = 4.00s
loss = 997.2635498046875, steps = 8400
global_step/sec: 25.3625
loss = 1135.93505859375, steps = 8500, cost time = 3.94s
loss = 1135.93505859375, steps = 8500
global_step/sec: 25.4025
loss = 1174.14892578125, steps = 8600, cost time = 3.94s
loss = 1174.14892578125, steps = 8600
global_step/sec: 24.5513
loss = 1218.946044921875, steps = 8700, cost time = 4.07s
loss = 1218.946044921875, steps = 8700
global_step/sec: 24.9860
loss = 1218.945556640625, steps = 8800, cost time = 4.00s
loss = 1218.945556640625, steps = 8800
global_step/sec: 24.8270
loss = 1110.52978515625, steps = 8900, cost time = 4.03s
loss = 1110.52978515625, steps = 8900
global_step/sec: 24.8994
loss = 959.873291015625, steps = 9000, cost time = 4.02s
loss = 959.873291015625, steps = 9000
global_step/sec: 25.4652
loss = 1206.1197509765625, steps = 9100, cost time = 3.93s
loss = 1206.1197509765625, steps = 9100
global_step/sec: 25.0173
loss = 1296.245849609375, steps = 9200, cost time = 4.00s
loss = 1296.245849609375, steps = 9200
global_step/sec: 25.5414
loss = 1091.53662109375, steps = 9300, cost time = 3.92s
loss = 1091.53662109375, steps = 9300
global_step/sec: 25.1220
loss = 1167.750732421875, steps = 9400, cost time = 3.98s
loss = 1167.750732421875, steps = 9400
global_step/sec: 24.6668
loss = 1276.857666015625, steps = 9500, cost time = 4.05s
loss = 1276.857666015625, steps = 9500
global_step/sec: 25.3203
loss = 1257.5164794921875, steps = 9600, cost time = 3.95s
loss = 1257.5164794921875, steps = 9600
global_step/sec: 24.4598
loss = 1296.239990234375, steps = 9700, cost time = 4.09s
loss = 1296.239990234375, steps = 9700
global_step/sec: 25.0347
loss = 1072.583984375, steps = 9800, cost time = 3.99s
loss = 1072.583984375, steps = 9800
global_step/sec: 25.6345
loss = 1110.5140380859375, steps = 9900, cost time = 3.90s
loss = 1110.5140380859375, steps = 9900
global_step/sec: 25.6276
loss = 1135.9049072265625, steps = 10000, cost time = 3.90s
loss = 1135.9049072265625, steps = 10000
global_step/sec: 24.5993
loss = 1432.989501953125, steps = 10100, cost time = 4.07s
loss = 1432.989501953125, steps = 10100
global_step/sec: 24.9901
loss = 1123.196044921875, steps = 10200, cost time = 4.00s
loss = 1123.196044921875, steps = 10200
global_step/sec: 25.5423
loss = 1104.18212890625, steps = 10300, cost time = 3.92s
loss = 1104.18212890625, steps = 10300
global_step/sec: 24.7891
loss = 1212.512939453125, steps = 10400, cost time = 4.03s
loss = 1212.512939453125, steps = 10400
global_step/sec: 24.3673
loss = 1034.8212890625, steps = 10500, cost time = 4.10s
loss = 1034.8212890625, steps = 10500
global_step/sec: 24.7948
loss = 1148.621337890625, steps = 10600, cost time = 4.03s
loss = 1148.621337890625, steps = 10600
global_step/sec: 25.2905
loss = 1110.509033203125, steps = 10700, cost time = 3.95s
loss = 1110.509033203125, steps = 10700
global_step/sec: 24.7553
loss = 1059.966796875, steps = 10800, cost time = 4.04s
loss = 1059.966796875, steps = 10800
global_step/sec: 25.3717
loss = 1186.8944091796875, steps = 10900, cost time = 3.94s
loss = 1186.8944091796875, steps = 10900
global_step/sec: 24.9827
loss = 1097.8402099609375, steps = 11000, cost time = 4.00s
loss = 1097.8402099609375, steps = 11000
global_step/sec: 25.0606
loss = 1034.80859375, steps = 11100, cost time = 3.99s
loss = 1034.80859375, steps = 11100
global_step/sec: 25.1889
loss = 1148.618408203125, steps = 11200, cost time = 3.97s
loss = 1148.618408203125, steps = 11200
global_step/sec: 24.9754
loss = 1078.8773193359375, steps = 11300, cost time = 4.00s
loss = 1078.8773193359375, steps = 11300
global_step/sec: 24.7936
loss = 1148.616943359375, steps = 11400, cost time = 4.03s
loss = 1148.616943359375, steps = 11400
global_step/sec: 24.7894
loss = 1180.497802734375, steps = 11500, cost time = 4.03s
loss = 1180.497802734375, steps = 11500
global_step/sec: 25.1307
loss = 1193.28857421875, steps = 11600, cost time = 3.98s
loss = 1193.28857421875, steps = 11600
global_step/sec: 25.1193
loss = 1116.8428955078125, steps = 11700, cost time = 3.98s
loss = 1116.8428955078125, steps = 11700
global_step/sec: 25.4753
loss = 1167.732421875, steps = 11800, cost time = 3.93s
loss = 1167.732421875, steps = 11800
global_step/sec: 25.1890
loss = 1154.978515625, steps = 11900, cost time = 3.97s
loss = 1154.978515625, steps = 11900
global_step/sec: 25.1238
loss = 1174.11083984375, steps = 12000, cost time = 3.98s
loss = 1174.11083984375, steps = 12000
global_step/sec: 24.6607
loss = 1174.1090087890625, steps = 12100, cost time = 4.06s
loss = 1174.1090087890625, steps = 12100
global_step/sec: 25.0816
loss = 1193.2860107421875, steps = 12200, cost time = 3.99s
loss = 1193.2860107421875, steps = 12200
global_step/sec: 55983.9675
loss = 24.580032348632812, steps = 12207, cost time = 0.22s
loss = 24.580032348632812, steps = 12207
Evaluation complate:[3/3]
ACC = 0.9466999769210815
AUC = 0.5039067268371582
