WARNING:tensorflow:From train.py:18: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.

WARNING:tensorflow:From train.py:18: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.

WARNING:tensorflow:From train.py:522: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

INFO:tensorflow:vocabulary_size = 543060 in UID is inferred from the number of elements in the vocabulary_file ./data/uid_voc.txt.
INFO:tensorflow:vocabulary_size = 367983 in ITEM is inferred from the number of elements in the vocabulary_file ./data/mid_voc.txt.
INFO:tensorflow:vocabulary_size = 1601 in CATEGORY is inferred from the number of elements in the vocabulary_file ./data/cat_voc.txt.
INFO:tensorflow:vocabulary_size = 367983 in HISTORY_ITEM is inferred from the number of elements in the vocabulary_file ./data/mid_voc.txt.
INFO:tensorflow:vocabulary_size = 1601 in HISTORY_CATEGORY is inferred from the number of elements in the vocabulary_file ./data/cat_voc.txt.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/util/random_seed.py:58: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.

INFO:tensorflow:Parsing ./data/local_train_splitByUser
INFO:tensorflow:Parsing ./data/local_test_splitByUser
WARNING:tensorflow:From train.py:535: The name tf.data.Iterator is deprecated. Please use tf.compat.v1.data.Iterator instead.

WARNING:tensorflow:From train.py:535: DatasetV1.output_types (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.data.get_output_types(dataset)`.
WARNING:tensorflow:From train.py:536: DatasetV1.output_shapes (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.data.get_output_shapes(dataset)`.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/ops/iterator_ops.py:347: Iterator.output_types (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.data.get_output_types(iterator)`.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/ops/iterator_ops.py:348: Iterator.output_shapes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.data.get_output_shapes(iterator)`.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/ops/iterator_ops.py:350: Iterator.output_classes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.data.get_output_classes(iterator)`.
WARNING:tensorflow:From train.py:362: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From train.py:364: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.

WARNING:tensorflow:From train.py:306: The name tf.feature_column.input_layer is deprecated. Please use tf.compat.v1.feature_column.input_layer instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/feature_column/feature_column.py:206: EmbeddingColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.
Instructions for updating:
The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/feature_column/feature_column_v2.py:3182: VocabularyFileCategoricalColumn._get_sparse_tensors (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.
Instructions for updating:
The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/feature_column/feature_column.py:2158: VocabularyFileCategoricalColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.
Instructions for updating:
The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/feature_column/feature_column_v2.py:3122: VocabularyFileCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.
Instructions for updating:
The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/feature_column/feature_column.py:207: EmbeddingColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.
Instructions for updating:
The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.
WARNING:tensorflow:From train.py:310: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

WARNING:tensorflow:From train.py:283: SequenceCategoricalColumn._get_sparse_tensors (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.
Instructions for updating:
The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.
WARNING:tensorflow:From train.py:46: The name tf.debugging.assert_equal is deprecated. Please use tf.compat.v1.debugging.assert_equal instead.

WARNING:tensorflow:From train.py:206: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.Dense instead.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
WARNING:tensorflow:From train.py:382: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.

WARNING:tensorflow:From train.py:347: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).
WARNING:tensorflow:From train.py:34: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From train.py:405: The name tf.losses.sigmoid_cross_entropy is deprecated. Please use tf.compat.v1.losses.sigmoid_cross_entropy instead.

WARNING:tensorflow:From train.py:409: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.

WARNING:tensorflow:From train.py:412: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.

WARNING:tensorflow:From train.py:413: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From train.py:169: The name tf.metrics.accuracy is deprecated. Please use tf.compat.v1.metrics.accuracy instead.

WARNING:tensorflow:From train.py:172: The name tf.metrics.auc is deprecated. Please use tf.compat.v1.metrics.auc instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/metrics_impl.py:808: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
WARNING:tensorflow:From train.py:564: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From train.py:599: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2022-03-24 07:57:44.255786: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2022-03-24 07:57:44.290844: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3400000000 Hz
2022-03-24 07:57:44.313143: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4a7c100 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-03-24 07:57:44.313172: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
WARNING:tensorflow:From train.py:600: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.

WARNING:tensorflow:From train.py:601: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.

WARNING:tensorflow:From train.py:602: The name tf.tables_initializer is deprecated. Please use tf.compat.v1.tables_initializer instead.

WARNING:tensorflow:From train.py:603: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

WARNING:tensorflow:From train.py:604: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

WARNING:tensorflow:From train.py:605: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

WARNING:tensorflow:From train.py:605: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

WARNING:tensorflow:From train.py:607: The name tf.RunOptions is deprecated. Please use tf.compat.v1.RunOptions instead.

WARNING:tensorflow:From train.py:608: The name tf.RunMetadata is deprecated. Please use tf.compat.v1.RunMetadata instead.

Using TensorFlow version 1.15.5
Checking dataset
Numbers of training dataset is 1086120
Numbers of test dataset is 121216
Saving model checkpoints to /benchmark_result/checkpoint/2022-03-24-15-57-35/din_tf_fp32
global_step/sec: 166.9384
loss = 0.6995416879653931, steps = 0, cost time = 0.60s
global_step/sec: 27.0304
loss = 0.6959691047668457, steps = 100, cost time = 3.70s
global_step/sec: 26.8480
loss = 0.6905705332756042, steps = 200, cost time = 3.72s
global_step/sec: 25.0867
loss = 0.6714383959770203, steps = 300, cost time = 3.99s
global_step/sec: 24.9987
loss = 0.6655911207199097, steps = 400, cost time = 4.00s
global_step/sec: 24.2889
loss = 0.667839527130127, steps = 500, cost time = 4.12s
global_step/sec: 25.8005
loss = 0.6433604955673218, steps = 600, cost time = 3.88s
global_step/sec: 26.1653
loss = 0.6367676258087158, steps = 700, cost time = 3.82s
global_step/sec: 26.5514
loss = 0.6245954036712646, steps = 800, cost time = 3.77s
global_step/sec: 24.6180
loss = 0.6488611698150635, steps = 900, cost time = 4.06s
global_step/sec: 25.9540
loss = 0.6613406538963318, steps = 1000, cost time = 3.85s
global_step/sec: 23.3028
loss = 0.6904213428497314, steps = 1100, cost time = 4.29s
global_step/sec: 23.8314
loss = 0.6358414888381958, steps = 1200, cost time = 4.20s
global_step/sec: 24.6228
loss = 0.6024055480957031, steps = 1300, cost time = 4.06s
global_step/sec: 27.7409
loss = 0.6612123847007751, steps = 1400, cost time = 3.60s
global_step/sec: 28.1694
loss = 0.6670020818710327, steps = 1500, cost time = 3.55s
global_step/sec: 25.0027
loss = 0.6364540457725525, steps = 1600, cost time = 4.00s
global_step/sec: 29.7848
loss = 0.6460645794868469, steps = 1700, cost time = 3.36s
global_step/sec: 23.9022
loss = 0.6132594347000122, steps = 1800, cost time = 4.18s
global_step/sec: 24.7808
loss = 0.6385817527770996, steps = 1900, cost time = 4.04s
global_step/sec: 25.4977
loss = 0.6738662123680115, steps = 2000, cost time = 3.92s
global_step/sec: 24.7767
loss = 0.6396483778953552, steps = 2100, cost time = 4.04s
global_step/sec: 23.7301
loss = 0.6078884601593018, steps = 2200, cost time = 4.21s
global_step/sec: 24.4417
loss = 0.6308271288871765, steps = 2300, cost time = 4.09s
global_step/sec: 25.9523
loss = 0.616041898727417, steps = 2400, cost time = 3.85s
global_step/sec: 26.0042
loss = 0.6460883617401123, steps = 2500, cost time = 3.85s
global_step/sec: 23.8330
loss = 0.5836215019226074, steps = 2600, cost time = 4.20s
global_step/sec: 24.9059
loss = 0.6825923919677734, steps = 2700, cost time = 4.02s
global_step/sec: 24.6577
loss = 0.5983253717422485, steps = 2800, cost time = 4.06s
global_step/sec: 27.6301
loss = 0.5998584628105164, steps = 2900, cost time = 3.62s
global_step/sec: 25.8269
loss = 0.5964800119400024, steps = 3000, cost time = 3.87s
global_step/sec: 26.2930
loss = 0.6361870765686035, steps = 3100, cost time = 3.80s
global_step/sec: 26.7474
loss = 0.5947076082229614, steps = 3200, cost time = 3.74s
global_step/sec: 24.6310
loss = 0.6004889607429504, steps = 3300, cost time = 4.06s
global_step/sec: 24.4713
loss = 0.6157020330429077, steps = 3400, cost time = 4.09s
global_step/sec: 28.8896
loss = 0.6114852428436279, steps = 3500, cost time = 3.46s
global_step/sec: 22.3651
loss = 0.5964833498001099, steps = 3600, cost time = 4.47s
global_step/sec: 28.7046
loss = 0.6032770872116089, steps = 3700, cost time = 3.48s
global_step/sec: 23.4178
loss = 0.5895002484321594, steps = 3800, cost time = 4.27s
global_step/sec: 24.4862
loss = 0.5643669366836548, steps = 3900, cost time = 4.08s
global_step/sec: 24.0445
loss = 0.5593594312667847, steps = 4000, cost time = 4.16s
global_step/sec: 22.9969
loss = 0.6354705095291138, steps = 4100, cost time = 4.35s
global_step/sec: 25.1916
loss = 0.6522519588470459, steps = 4200, cost time = 3.97s
global_step/sec: 25.1456
loss = 0.5731474161148071, steps = 4300, cost time = 3.98s
global_step/sec: 22.4076
loss = 0.5854809284210205, steps = 4400, cost time = 4.46s
global_step/sec: 24.1403
loss = 0.5701040625572205, steps = 4500, cost time = 4.14s
global_step/sec: 26.0714
loss = 0.5454152822494507, steps = 4600, cost time = 3.84s
global_step/sec: 25.3445
loss = 0.6705358028411865, steps = 4700, cost time = 3.95s
global_step/sec: 23.2860
loss = 0.585027813911438, steps = 4800, cost time = 4.29s
global_step/sec: 22.6887
loss = 0.5880990624427795, steps = 4900, cost time = 4.41s
global_step/sec: 23.4088
loss = 0.5482616424560547, steps = 5000, cost time = 4.27s
global_step/sec: 23.1266
loss = 0.6186522841453552, steps = 5100, cost time = 4.32s
global_step/sec: 24.3809
loss = 0.5686585903167725, steps = 5200, cost time = 4.10s
global_step/sec: 24.8355
loss = 0.5992553234100342, steps = 5300, cost time = 4.03s
global_step/sec: 23.3896
loss = 0.546460747718811, steps = 5400, cost time = 4.28s
global_step/sec: 23.3566
loss = 0.5551329851150513, steps = 5500, cost time = 4.28s
global_step/sec: 23.1450
loss = 0.5345717668533325, steps = 5600, cost time = 4.32s
global_step/sec: 23.1605
loss = 0.5724208354949951, steps = 5700, cost time = 4.32s
global_step/sec: 23.9734
loss = 0.5215462446212769, steps = 5800, cost time = 4.17s
global_step/sec: 24.1490
loss = 0.5413782596588135, steps = 5900, cost time = 4.14s
global_step/sec: 24.4463
loss = 0.5949292182922363, steps = 6000, cost time = 4.09s
global_step/sec: 25.5422
loss = 0.594144344329834, steps = 6100, cost time = 3.92s
global_step/sec: 25.9733
loss = 0.5619098544120789, steps = 6200, cost time = 3.85s
global_step/sec: 21.9264
loss = 0.6392866373062134, steps = 6300, cost time = 4.56s
global_step/sec: 26.3787
loss = 0.587418794631958, steps = 6400, cost time = 3.79s
global_step/sec: 24.4699
loss = 0.6052260398864746, steps = 6500, cost time = 4.09s
global_step/sec: 25.6084
loss = 0.6030850410461426, steps = 6600, cost time = 3.90s
global_step/sec: 24.2956
loss = 0.5755869150161743, steps = 6700, cost time = 4.12s
global_step/sec: 23.7854
loss = 0.57465660572052, steps = 6800, cost time = 4.20s
global_step/sec: 24.8175
loss = 0.5443271398544312, steps = 6900, cost time = 4.03s
global_step/sec: 24.6290
loss = 0.5360362529754639, steps = 7000, cost time = 4.06s
global_step/sec: 25.2251
loss = 0.5635586977005005, steps = 7100, cost time = 3.96s
global_step/sec: 22.6736
loss = 0.6540926098823547, steps = 7200, cost time = 4.41s
global_step/sec: 25.5223
loss = 0.5634212493896484, steps = 7300, cost time = 3.92s
global_step/sec: 26.9980
loss = 0.6446729898452759, steps = 7400, cost time = 3.70s
global_step/sec: 26.7744
loss = 0.6030600070953369, steps = 7500, cost time = 3.73s
global_step/sec: 23.1666
loss = 0.5649975538253784, steps = 7600, cost time = 4.32s
global_step/sec: 23.2546
loss = 0.6246720552444458, steps = 7700, cost time = 4.30s
global_step/sec: 23.4355
loss = 0.586645781993866, steps = 7800, cost time = 4.27s
global_step/sec: 22.9886
loss = 0.5306010842323303, steps = 7900, cost time = 4.35s
global_step/sec: 25.3488
loss = 0.5400834083557129, steps = 8000, cost time = 3.94s
global_step/sec: 23.4687
loss = 0.5968079566955566, steps = 8100, cost time = 4.26s
global_step/sec: 22.5711
loss = 0.5852361917495728, steps = 8200, cost time = 4.43s
global_step/sec: 22.6714
loss = 0.5649480819702148, steps = 8300, cost time = 4.41s
global_step/sec: 23.2595
loss = 0.6025967597961426, steps = 8400, cost time = 4.30s
global_step/sec: 24.1475
loss = 0.5774189233779907, steps = 8500, cost time = 4.14s
global_step/sec: 22.5123
loss = 0.5551120042800903, steps = 8600, cost time = 4.44s
global_step/sec: 23.3146
loss = 0.5498173832893372, steps = 8700, cost time = 4.29s
global_step/sec: 23.6531
loss = 0.4683133065700531, steps = 8800, cost time = 4.23s
global_step/sec: 22.2503
loss = 0.48658737540245056, steps = 8900, cost time = 4.49s
global_step/sec: 20.7848
loss = 0.47640132904052734, steps = 9000, cost time = 4.81s
global_step/sec: 20.9642
loss = 0.48185259103775024, steps = 9100, cost time = 4.77s
global_step/sec: 22.1570
loss = 0.45888322591781616, steps = 9200, cost time = 4.51s
global_step/sec: 21.7881
loss = 0.48416370153427124, steps = 9300, cost time = 4.59s
global_step/sec: 23.4960
loss = 0.3976304531097412, steps = 9400, cost time = 4.26s
global_step/sec: 23.3083
loss = 0.3985787034034729, steps = 9500, cost time = 4.29s
global_step/sec: 24.0046
loss = 0.41577768325805664, steps = 9600, cost time = 4.17s
global_step/sec: 23.6057
loss = 0.4614967405796051, steps = 9700, cost time = 4.24s
global_step/sec: 23.4597
loss = 0.33095574378967285, steps = 9800, cost time = 4.26s
global_step/sec: 25.6649
loss = 0.40067028999328613, steps = 9900, cost time = 3.90s
global_step/sec: 23.8881
loss = 0.3697549104690552, steps = 10000, cost time = 4.19s
global_step/sec: 24.4626
loss = 0.3835710883140564, steps = 10100, cost time = 4.09s
global_step/sec: 23.4608
loss = 0.4294392466545105, steps = 10200, cost time = 4.26s
global_step/sec: 25.0942
loss = 0.40185651183128357, steps = 10300, cost time = 3.98s
global_step/sec: 24.5626
loss = 0.38787391781806946, steps = 10400, cost time = 4.07s
global_step/sec: 23.7832
loss = 0.41491758823394775, steps = 10500, cost time = 4.20s
global_step/sec: 23.9873
loss = 0.43413370847702026, steps = 10600, cost time = 4.17s
global_step/sec: 23.7803
loss = 0.4457548260688782, steps = 10700, cost time = 4.21s
global_step/sec: 24.4637
loss = 0.4809796214103699, steps = 10800, cost time = 4.09s
global_step/sec: 23.7642
loss = 0.4640142023563385, steps = 10900, cost time = 4.21s
global_step/sec: 22.8419
loss = 0.4015747010707855, steps = 11000, cost time = 4.38s
global_step/sec: 23.9402
loss = 0.3903483748435974, steps = 11100, cost time = 4.18s
global_step/sec: 22.9432
loss = 0.3223404288291931, steps = 11200, cost time = 4.36s
global_step/sec: 23.2537
loss = 0.4036334156990051, steps = 11300, cost time = 4.30s
global_step/sec: 22.4675
loss = 0.5709999799728394, steps = 11400, cost time = 4.45s
global_step/sec: 22.7348
loss = 0.32693159580230713, steps = 11500, cost time = 4.40s
global_step/sec: 22.3615
loss = 0.4422542154788971, steps = 11600, cost time = 4.47s
global_step/sec: 22.0401
loss = 0.39057865738868713, steps = 11700, cost time = 4.54s
global_step/sec: 23.1339
loss = 0.4107232689857483, steps = 11800, cost time = 4.32s
global_step/sec: 21.7066
loss = 0.41405075788497925, steps = 11900, cost time = 4.61s
global_step/sec: 22.4814
loss = 0.34548720717430115, steps = 12000, cost time = 4.45s
global_step/sec: 22.9205
loss = 0.5236539244651794, steps = 12100, cost time = 4.36s
global_step/sec: 21.7129
loss = 0.4788060486316681, steps = 12200, cost time = 4.61s
global_step/sec: 22.1459
loss = 0.3414662182331085, steps = 12300, cost time = 4.52s
global_step/sec: 23.1000
loss = 0.35397714376449585, steps = 12400, cost time = 4.33s
global_step/sec: 22.3356
loss = 0.4287233054637909, steps = 12500, cost time = 4.48s
global_step/sec: 23.8093
loss = 0.33133983612060547, steps = 12600, cost time = 4.20s
global_step/sec: 22.3812
loss = 0.3812483549118042, steps = 12700, cost time = 4.47s
global_step/sec: 22.9963
loss = 0.49117305874824524, steps = 12800, cost time = 4.35s
global_step/sec: 23.0967
loss = 0.3995705842971802, steps = 12900, cost time = 4.33s
global_step/sec: 23.6703
loss = 0.3571871519088745, steps = 13000, cost time = 4.22s
global_step/sec: 23.6118
loss = 0.36935821175575256, steps = 13100, cost time = 4.24s
global_step/sec: 24.8117
loss = 0.3560446500778198, steps = 13200, cost time = 4.03s
global_step/sec: 22.4091
loss = 0.44478046894073486, steps = 13300, cost time = 4.46s
global_step/sec: 22.5153
loss = 0.4277734160423279, steps = 13400, cost time = 4.44s
global_step/sec: 23.5386
loss = 0.4325543940067291, steps = 13500, cost time = 4.25s
global_step/sec: 22.3654
loss = 0.4068971872329712, steps = 13600, cost time = 4.47s
global_step/sec: 22.2541
loss = 0.33707746863365173, steps = 13700, cost time = 4.49s
global_step/sec: 22.7460
loss = 0.3942680060863495, steps = 13800, cost time = 4.40s
global_step/sec: 24.3597
loss = 0.36665475368499756, steps = 13900, cost time = 4.11s
global_step/sec: 24.5406
loss = 0.3482060134410858, steps = 14000, cost time = 4.07s
global_step/sec: 23.3754
loss = 0.31347519159317017, steps = 14100, cost time = 4.28s
global_step/sec: 22.5094
loss = 0.40472692251205444, steps = 14200, cost time = 4.44s
global_step/sec: 23.2137
loss = 0.2842554748058319, steps = 14300, cost time = 4.31s
global_step/sec: 24.3171
loss = 0.33198797702789307, steps = 14400, cost time = 4.11s
global_step/sec: 23.1281
loss = 0.35753631591796875, steps = 14500, cost time = 4.32s
global_step/sec: 24.3356
loss = 0.46456754207611084, steps = 14600, cost time = 4.11s
global_step/sec: 22.6693
loss = 0.35062772035598755, steps = 14700, cost time = 4.41s
global_step/sec: 24.0566
loss = 0.3644835948944092, steps = 14800, cost time = 4.16s
global_step/sec: 22.7124
loss = 0.3296598792076111, steps = 14900, cost time = 4.40s
global_step/sec: 24.8502
loss = 0.4007723927497864, steps = 15000, cost time = 4.02s
global_step/sec: 24.6872
loss = 0.36860114336013794, steps = 15100, cost time = 4.05s
global_step/sec: 20.9713
loss = 0.32777512073516846, steps = 15200, cost time = 4.77s
global_step/sec: 23.0113
loss = 0.31128084659576416, steps = 15300, cost time = 4.35s
global_step/sec: 21.7412
loss = 0.3518916070461273, steps = 15400, cost time = 4.60s
global_step/sec: 22.4237
loss = 0.3371013402938843, steps = 15500, cost time = 4.46s
global_step/sec: 22.7682
loss = 0.30385807156562805, steps = 15600, cost time = 4.39s
global_step/sec: 21.9452
loss = 0.40826037526130676, steps = 15700, cost time = 4.56s
global_step/sec: 23.1995
loss = 0.32349327206611633, steps = 15800, cost time = 4.31s
global_step/sec: 22.9278
loss = 0.3738177418708801, steps = 15900, cost time = 4.36s
global_step/sec: 21.9281
loss = 0.4617360234260559, steps = 16000, cost time = 4.56s
global_step/sec: 24.0721
loss = 0.32706090807914734, steps = 16100, cost time = 4.15s
global_step/sec: 23.9828
loss = 0.325184166431427, steps = 16200, cost time = 4.17s
global_step/sec: 24.0198
loss = 0.3578340709209442, steps = 16300, cost time = 4.16s
global_step/sec: 22.8499
loss = 0.34262287616729736, steps = 16400, cost time = 4.38s
global_step/sec: 21.8752
loss = 0.3332570493221283, steps = 16500, cost time = 4.57s
global_step/sec: 21.8285
loss = 0.33736222982406616, steps = 16600, cost time = 4.58s
global_step/sec: 21.0969
loss = 0.37526047229766846, steps = 16700, cost time = 4.74s
global_step/sec: 22.5824
loss = 0.33149439096450806, steps = 16800, cost time = 4.43s
global_step/sec: 21.4676
loss = 0.4025557041168213, steps = 16900, cost time = 4.66s
global_step/sec: 22.7632
loss = 0.3054907023906708, steps = 17000, cost time = 4.39s
global_step/sec: 22.7359
loss = 0.3110324442386627, steps = 17100, cost time = 4.40s
global_step/sec: 22.2490
loss = 0.4471634030342102, steps = 17200, cost time = 4.49s
global_step/sec: 21.6373
loss = 0.276242196559906, steps = 17300, cost time = 4.62s
global_step/sec: 21.8167
loss = 0.3479384183883667, steps = 17400, cost time = 4.58s
global_step/sec: 22.9223
loss = 0.25598227977752686, steps = 17500, cost time = 4.36s
global_step/sec: 22.5725
loss = 0.292119562625885, steps = 17600, cost time = 4.43s
global_step/sec: 22.8300
loss = 0.2803399860858917, steps = 17700, cost time = 4.38s
global_step/sec: 23.9362
loss = 0.2368571162223816, steps = 17800, cost time = 4.18s
global_step/sec: 23.0458
loss = 0.23480048775672913, steps = 17900, cost time = 4.34s
global_step/sec: 22.5501
loss = 0.21355858445167542, steps = 18000, cost time = 4.43s
global_step/sec: 22.5173
loss = 0.17402774095535278, steps = 18100, cost time = 4.44s
global_step/sec: 24.4564
loss = 0.18496373295783997, steps = 18200, cost time = 4.09s
global_step/sec: 24.2693
loss = 0.24645818769931793, steps = 18300, cost time = 4.12s
global_step/sec: 22.2420
loss = 0.26512137055397034, steps = 18400, cost time = 4.50s
global_step/sec: 23.8585
loss = 0.23529799282550812, steps = 18500, cost time = 4.19s
global_step/sec: 23.9747
loss = 0.22979968786239624, steps = 18600, cost time = 4.17s
global_step/sec: 23.0017
loss = 0.23570258915424347, steps = 18700, cost time = 4.35s
global_step/sec: 22.5100
loss = 0.1911456435918808, steps = 18800, cost time = 4.44s
global_step/sec: 21.5065
loss = 0.226444810628891, steps = 18900, cost time = 4.65s
global_step/sec: 22.2186
loss = 0.18015289306640625, steps = 19000, cost time = 4.50s
global_step/sec: 23.7957
loss = 0.2306777983903885, steps = 19100, cost time = 4.20s
global_step/sec: 22.7581
loss = 0.11694984883069992, steps = 19200, cost time = 4.39s
global_step/sec: 22.6380
loss = 0.18700507283210754, steps = 19300, cost time = 4.42s
global_step/sec: 24.0037
loss = 0.22690963745117188, steps = 19400, cost time = 4.17s
global_step/sec: 22.7806
loss = 0.32294464111328125, steps = 19500, cost time = 4.39s
global_step/sec: 22.6519
loss = 0.2109590321779251, steps = 19600, cost time = 4.41s
global_step/sec: 23.8206
loss = 0.15155181288719177, steps = 19700, cost time = 4.20s
global_step/sec: 23.1697
loss = 0.2644146680831909, steps = 19800, cost time = 4.32s
global_step/sec: 23.6261
loss = 0.18915852904319763, steps = 19900, cost time = 4.23s
global_step/sec: 24.0538
loss = 0.21861234307289124, steps = 20000, cost time = 4.16s
global_step/sec: 23.3993
loss = 0.27530956268310547, steps = 20100, cost time = 4.27s
global_step/sec: 22.2911
loss = 0.17465339601039886, steps = 20200, cost time = 4.49s
global_step/sec: 22.4372
loss = 0.3449537754058838, steps = 20300, cost time = 4.46s
global_step/sec: 23.0789
loss = 0.14770227670669556, steps = 20400, cost time = 4.33s
global_step/sec: 23.9985
loss = 0.17100027203559875, steps = 20500, cost time = 4.17s
global_step/sec: 22.0087
loss = 0.2058149129152298, steps = 20600, cost time = 4.54s
global_step/sec: 22.2559
loss = 0.18191400170326233, steps = 20700, cost time = 4.49s
global_step/sec: 22.8765
loss = 0.17970529198646545, steps = 20800, cost time = 4.37s
global_step/sec: 21.8802
loss = 0.1850152611732483, steps = 20900, cost time = 4.57s
global_step/sec: 22.8954
loss = 0.13961292803287506, steps = 21000, cost time = 4.37s
global_step/sec: 22.4215
loss = 0.17750486731529236, steps = 21100, cost time = 4.46s
global_step/sec: 22.4378
loss = 0.31021130084991455, steps = 21200, cost time = 4.46s
global_step/sec: 23.1455
loss = 0.21263369917869568, steps = 21300, cost time = 4.32s
global_step/sec: 24.2873
loss = 0.1857905387878418, steps = 21400, cost time = 4.12s
global_step/sec: 23.5989
loss = 0.14074845612049103, steps = 21500, cost time = 4.24s
global_step/sec: 23.7556
loss = 0.10044384747743607, steps = 21600, cost time = 4.21s
global_step/sec: 23.2364
loss = 0.24418862164020538, steps = 21700, cost time = 4.30s
global_step/sec: 21.8443
loss = 0.1843702793121338, steps = 21800, cost time = 4.58s
global_step/sec: 22.9565
loss = 0.13638122379779816, steps = 21900, cost time = 4.36s
global_step/sec: 22.3791
loss = 0.17092397809028625, steps = 22000, cost time = 4.47s
global_step/sec: 22.2200
loss = 0.24773535132408142, steps = 22100, cost time = 4.50s
global_step/sec: 24.1109
loss = 0.17939156293869019, steps = 22200, cost time = 4.15s
global_step/sec: 24.1833
loss = 0.17663830518722534, steps = 22300, cost time = 4.14s
global_step/sec: 23.4856
loss = 0.12331384420394897, steps = 22400, cost time = 4.26s
global_step/sec: 22.9966
loss = 0.21575012803077698, steps = 22500, cost time = 4.35s
global_step/sec: 21.6465
loss = 0.18856438994407654, steps = 22600, cost time = 4.62s
global_step/sec: 25.6298
loss = 0.16300003230571747, steps = 22700, cost time = 3.90s
global_step/sec: 23.7761
loss = 0.20763595402240753, steps = 22800, cost time = 4.21s
global_step/sec: 23.0735
loss = 0.1415155977010727, steps = 22900, cost time = 4.33s
global_step/sec: 23.5524
loss = 0.11191820353269577, steps = 23000, cost time = 4.25s
global_step/sec: 23.1541
loss = 0.11007514595985413, steps = 23100, cost time = 4.32s
global_step/sec: 23.9266
loss = 0.16832832992076874, steps = 23200, cost time = 4.18s
global_step/sec: 23.9077
loss = 0.2585407495498657, steps = 23300, cost time = 4.18s
global_step/sec: 22.9088
loss = 0.18524545431137085, steps = 23400, cost time = 4.37s
global_step/sec: 24.8896
loss = 0.13074100017547607, steps = 23500, cost time = 4.02s
global_step/sec: 24.4531
loss = 0.13119958341121674, steps = 23600, cost time = 4.09s
global_step/sec: 26.0304
loss = 0.12391814589500427, steps = 23700, cost time = 3.84s
global_step/sec: 24.7005
loss = 0.09963670372962952, steps = 23800, cost time = 4.05s
global_step/sec: 25.9706
loss = 0.1124187707901001, steps = 23900, cost time = 3.85s
global_step/sec: 24.6654
loss = 0.160958930850029, steps = 24000, cost time = 4.05s
global_step/sec: 26.8452
loss = 0.1497086137533188, steps = 24100, cost time = 3.73s
global_step/sec: 24.1766
loss = 0.1339113712310791, steps = 24200, cost time = 4.14s
global_step/sec: 22.5591
loss = 0.09096567332744598, steps = 24300, cost time = 4.43s
global_step/sec: 22.5354
loss = 0.16815584897994995, steps = 24400, cost time = 4.44s
global_step/sec: 23.7454
loss = 0.11125700920820236, steps = 24500, cost time = 4.21s
global_step/sec: 24.4933
loss = 0.10662683099508286, steps = 24600, cost time = 4.08s
global_step/sec: 26.0902
loss = 0.08376020193099976, steps = 24700, cost time = 3.83s
global_step/sec: 23.4013
loss = 0.13808254897594452, steps = 24800, cost time = 4.27s
global_step/sec: 23.6634
loss = 0.09525422006845474, steps = 24900, cost time = 4.23s
global_step/sec: 26.1305
loss = 0.06826537847518921, steps = 25000, cost time = 3.83s
global_step/sec: 26.0958
loss = 0.1231331005692482, steps = 25100, cost time = 3.83s
global_step/sec: 23.1970
loss = 0.10071912407875061, steps = 25200, cost time = 4.31s
global_step/sec: 22.3880
loss = 0.10422783344984055, steps = 25300, cost time = 4.47s
global_step/sec: 20.0917
loss = 0.17938965559005737, steps = 25400, cost time = 4.98s
global_step/sec: 9850.6961
loss = 0.15199285745620728, steps = 25455, cost time = 2.58s
Evaluation complate:[947/947]
ACC = 0.6792832612991333
AUC = 0.7437077760696411
