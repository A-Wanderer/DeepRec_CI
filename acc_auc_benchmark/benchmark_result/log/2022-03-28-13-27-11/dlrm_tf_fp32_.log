WARNING:tensorflow:From train.py:15: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.

WARNING:tensorflow:From train.py:15: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.

WARNING:tensorflow:From train.py:381: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/util/random_seed.py:58: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.

INFO:tensorflow:Parsing ./data/train.csv
INFO:tensorflow:Parsing ./data/eval.csv
WARNING:tensorflow:From train.py:394: The name tf.data.Iterator is deprecated. Please use tf.compat.v1.data.Iterator instead.

WARNING:tensorflow:From train.py:394: DatasetV1.output_types (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.data.get_output_types(dataset)`.
WARNING:tensorflow:From train.py:395: DatasetV1.output_shapes (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.data.get_output_shapes(dataset)`.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/ops/iterator_ops.py:347: Iterator.output_types (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.data.get_output_types(iterator)`.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/ops/iterator_ops.py:348: Iterator.output_shapes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.data.get_output_shapes(iterator)`.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/ops/iterator_ops.py:350: Iterator.output_classes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.data.get_output_classes(iterator)`.
WARNING:tensorflow:From train.py:168: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From train.py:168: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.

WARNING:tensorflow:From train.py:172: The name tf.feature_column.input_layer is deprecated. Please use tf.compat.v1.feature_column.input_layer instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/feature_column/feature_column.py:206: NumericColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.
Instructions for updating:
The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/feature_column/feature_column.py:2158: NumericColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.
Instructions for updating:
The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/feature_column/feature_column.py:207: NumericColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.
Instructions for updating:
The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/feature_column/feature_column.py:206: EmbeddingColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.
Instructions for updating:
The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/feature_column/feature_column_v2.py:3182: HashedCategoricalColumn._get_sparse_tensors (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.
Instructions for updating:
The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/feature_column/feature_column.py:2158: HashedCategoricalColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.
Instructions for updating:
The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/feature_column/feature_column_v2.py:3122: HashedCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.
Instructions for updating:
The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/feature_column/feature_column.py:207: EmbeddingColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.
Instructions for updating:
The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.
WARNING:tensorflow:From train.py:199: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.Dense instead.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
WARNING:tensorflow:From train.py:203: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).
WARNING:tensorflow:From train.py:54: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From train.py:56: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.

WARNING:tensorflow:From train.py:262: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.

WARNING:tensorflow:From train.py:263: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.

WARNING:tensorflow:From train.py:144: The name tf.metrics.accuracy is deprecated. Please use tf.compat.v1.metrics.accuracy instead.

WARNING:tensorflow:From train.py:147: The name tf.metrics.auc is deprecated. Please use tf.compat.v1.metrics.auc instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/metrics_impl.py:808: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
WARNING:tensorflow:From train.py:422: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From train.py:455: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2022-03-28 05:27:21.079327: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2022-03-28 05:27:21.096740: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3400000000 Hz
2022-03-28 05:27:21.099032: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3acc470 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-03-28 05:27:21.099052: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
WARNING:tensorflow:From train.py:456: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.

WARNING:tensorflow:From train.py:457: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.

WARNING:tensorflow:From train.py:458: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

WARNING:tensorflow:From train.py:459: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

WARNING:tensorflow:From train.py:460: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

WARNING:tensorflow:From train.py:460: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

WARNING:tensorflow:From train.py:462: The name tf.RunOptions is deprecated. Please use tf.compat.v1.RunOptions instead.

WARNING:tensorflow:From train.py:463: The name tf.RunMetadata is deprecated. Please use tf.compat.v1.RunMetadata instead.

Using TensorFlow version 1.15.5
Checking dataset
Numbers of training dataset is 8000000
Numbers of test dataset is 2000000
Saving model checkpoints to /benchmark_result/checkpoint/2022-03-28-13-27-11/dlrm_tf_fp32_
global_step/sec: 110.1250
loss = 0.7316404581069946, steps = 0, cost time = 0.91s
global_step/sec: 95.5438
loss = 0.4929015338420868, steps = 100, cost time = 1.05s
global_step/sec: 84.5454
loss = 0.45740610361099243, steps = 200, cost time = 1.18s
global_step/sec: 83.6094
loss = 0.5421258211135864, steps = 300, cost time = 1.20s
global_step/sec: 82.5240
loss = 0.5365016460418701, steps = 400, cost time = 1.21s
global_step/sec: 83.0257
loss = 0.501331090927124, steps = 500, cost time = 1.20s
global_step/sec: 83.6608
loss = 0.5608423352241516, steps = 600, cost time = 1.20s
global_step/sec: 83.3814
loss = 0.5596646666526794, steps = 700, cost time = 1.20s
global_step/sec: 82.1291
loss = 0.5400905609130859, steps = 800, cost time = 1.22s
global_step/sec: 81.6943
loss = 0.5074447393417358, steps = 900, cost time = 1.22s
global_step/sec: 82.4693
loss = 0.5410280227661133, steps = 1000, cost time = 1.21s
global_step/sec: 83.4658
loss = 0.5435562133789062, steps = 1100, cost time = 1.20s
global_step/sec: 82.2732
loss = 0.4963504374027252, steps = 1200, cost time = 1.22s
global_step/sec: 81.2613
loss = 0.4836583137512207, steps = 1300, cost time = 1.23s
global_step/sec: 82.8871
loss = 0.49647215008735657, steps = 1400, cost time = 1.21s
global_step/sec: 82.9075
loss = 0.4791022539138794, steps = 1500, cost time = 1.21s
global_step/sec: 83.2342
loss = 0.48501747846603394, steps = 1600, cost time = 1.20s
global_step/sec: 82.3387
loss = 0.5194010734558105, steps = 1700, cost time = 1.21s
global_step/sec: 81.4784
loss = 0.4959850609302521, steps = 1800, cost time = 1.23s
global_step/sec: 82.5237
loss = 0.4997425079345703, steps = 1900, cost time = 1.21s
global_step/sec: 81.5544
loss = 0.4752786159515381, steps = 2000, cost time = 1.23s
global_step/sec: 81.0742
loss = 0.4929353594779968, steps = 2100, cost time = 1.23s
global_step/sec: 81.3209
loss = 0.5261961221694946, steps = 2200, cost time = 1.23s
global_step/sec: 82.7476
loss = 0.481719434261322, steps = 2300, cost time = 1.21s
global_step/sec: 82.1061
loss = 0.5141857862472534, steps = 2400, cost time = 1.22s
global_step/sec: 82.1790
loss = 0.4696197211742401, steps = 2500, cost time = 1.22s
global_step/sec: 81.8890
loss = 0.5203865766525269, steps = 2600, cost time = 1.22s
global_step/sec: 82.6629
loss = 0.5334843397140503, steps = 2700, cost time = 1.21s
global_step/sec: 80.7653
loss = 0.5095135569572449, steps = 2800, cost time = 1.24s
global_step/sec: 82.1162
loss = 0.5241810083389282, steps = 2900, cost time = 1.22s
global_step/sec: 81.9329
loss = 0.5079456567764282, steps = 3000, cost time = 1.22s
global_step/sec: 82.9489
loss = 0.5184878706932068, steps = 3100, cost time = 1.21s
global_step/sec: 83.0830
loss = 0.49959275126457214, steps = 3200, cost time = 1.20s
global_step/sec: 82.0441
loss = 0.4900176525115967, steps = 3300, cost time = 1.22s
global_step/sec: 84.2002
loss = 0.48997193574905396, steps = 3400, cost time = 1.19s
global_step/sec: 81.7760
loss = 0.5044381618499756, steps = 3500, cost time = 1.22s
global_step/sec: 82.2434
loss = 0.4740428626537323, steps = 3600, cost time = 1.22s
global_step/sec: 81.5308
loss = 0.489193856716156, steps = 3700, cost time = 1.23s
global_step/sec: 82.6686
loss = 0.49476760625839233, steps = 3800, cost time = 1.21s
global_step/sec: 83.0011
loss = 0.5033385753631592, steps = 3900, cost time = 1.20s
global_step/sec: 82.4614
loss = 0.5423591136932373, steps = 4000, cost time = 1.21s
global_step/sec: 82.9032
loss = 0.48745596408843994, steps = 4100, cost time = 1.21s
global_step/sec: 82.7243
loss = 0.4705204963684082, steps = 4200, cost time = 1.21s
global_step/sec: 81.9732
loss = 0.5188319683074951, steps = 4300, cost time = 1.22s
global_step/sec: 82.8459
loss = 0.5004124045372009, steps = 4400, cost time = 1.21s
global_step/sec: 83.6440
loss = 0.4918951392173767, steps = 4500, cost time = 1.20s
global_step/sec: 82.6400
loss = 0.48611894249916077, steps = 4600, cost time = 1.21s
global_step/sec: 82.1221
loss = 0.45702192187309265, steps = 4700, cost time = 1.22s
global_step/sec: 83.8711
loss = 0.48799756169319153, steps = 4800, cost time = 1.19s
global_step/sec: 83.3706
loss = 0.4828415811061859, steps = 4900, cost time = 1.20s
global_step/sec: 81.8189
loss = 0.4877021312713623, steps = 5000, cost time = 1.22s
global_step/sec: 82.8208
loss = 0.49145418405532837, steps = 5100, cost time = 1.21s
global_step/sec: 82.8561
loss = 0.49459129571914673, steps = 5200, cost time = 1.21s
global_step/sec: 82.8292
loss = 0.4732678532600403, steps = 5300, cost time = 1.21s
global_step/sec: 81.8095
loss = 0.5274960994720459, steps = 5400, cost time = 1.22s
global_step/sec: 82.7315
loss = 0.5112829208374023, steps = 5500, cost time = 1.21s
global_step/sec: 82.5276
loss = 0.48157814145088196, steps = 5600, cost time = 1.21s
global_step/sec: 83.3723
loss = 0.5055149793624878, steps = 5700, cost time = 1.20s
global_step/sec: 82.7913
loss = 0.5125144720077515, steps = 5800, cost time = 1.21s
global_step/sec: 83.4169
loss = 0.5176886320114136, steps = 5900, cost time = 1.20s
global_step/sec: 83.5529
loss = 0.4789574146270752, steps = 6000, cost time = 1.20s
global_step/sec: 83.5189
loss = 0.5271207094192505, steps = 6100, cost time = 1.20s
global_step/sec: 83.6412
loss = 0.48939022421836853, steps = 6200, cost time = 1.20s
global_step/sec: 84.0600
loss = 0.5210607051849365, steps = 6300, cost time = 1.19s
global_step/sec: 83.3220
loss = 0.5329722166061401, steps = 6400, cost time = 1.20s
global_step/sec: 83.2999
loss = 0.4719391465187073, steps = 6500, cost time = 1.20s
global_step/sec: 83.7156
loss = 0.48291197419166565, steps = 6600, cost time = 1.19s
global_step/sec: 82.7167
loss = 0.48526328802108765, steps = 6700, cost time = 1.21s
global_step/sec: 83.1744
loss = 0.485363632440567, steps = 6800, cost time = 1.20s
global_step/sec: 83.4194
loss = 0.43935835361480713, steps = 6900, cost time = 1.20s
global_step/sec: 82.7788
loss = 0.5151432156562805, steps = 7000, cost time = 1.21s
global_step/sec: 83.5573
loss = 0.5498297810554504, steps = 7100, cost time = 1.20s
global_step/sec: 83.5775
loss = 0.48583489656448364, steps = 7200, cost time = 1.20s
global_step/sec: 83.6560
loss = 0.4936732053756714, steps = 7300, cost time = 1.20s
global_step/sec: 83.7605
loss = 0.4891429543495178, steps = 7400, cost time = 1.19s
global_step/sec: 83.8504
loss = 0.48675063252449036, steps = 7500, cost time = 1.19s
global_step/sec: 83.6561
loss = 0.49473994970321655, steps = 7600, cost time = 1.20s
global_step/sec: 83.2420
loss = 0.43003031611442566, steps = 7700, cost time = 1.20s
global_step/sec: 84.5730
loss = 0.43629151582717896, steps = 7800, cost time = 1.18s
global_step/sec: 84.0128
loss = 0.4701145589351654, steps = 7900, cost time = 1.19s
global_step/sec: 84.6605
loss = 0.45253777503967285, steps = 8000, cost time = 1.18s
global_step/sec: 83.3245
loss = 0.4706985056400299, steps = 8100, cost time = 1.20s
global_step/sec: 83.5112
loss = 0.47940075397491455, steps = 8200, cost time = 1.20s
global_step/sec: 84.2049
loss = 0.4573413133621216, steps = 8300, cost time = 1.19s
global_step/sec: 83.2986
loss = 0.4883502125740051, steps = 8400, cost time = 1.20s
global_step/sec: 83.9605
loss = 0.49777233600616455, steps = 8500, cost time = 1.19s
global_step/sec: 83.9497
loss = 0.47330087423324585, steps = 8600, cost time = 1.19s
global_step/sec: 84.3909
loss = 0.4634871780872345, steps = 8700, cost time = 1.18s
global_step/sec: 83.4698
loss = 0.4908498525619507, steps = 8800, cost time = 1.20s
global_step/sec: 84.7662
loss = 0.4760289192199707, steps = 8900, cost time = 1.18s
global_step/sec: 84.3235
loss = 0.48824262619018555, steps = 9000, cost time = 1.19s
global_step/sec: 83.2574
loss = 0.5017218589782715, steps = 9100, cost time = 1.20s
global_step/sec: 84.8171
loss = 0.47752857208251953, steps = 9200, cost time = 1.18s
global_step/sec: 82.3590
loss = 0.5054552555084229, steps = 9300, cost time = 1.21s
global_step/sec: 82.6187
loss = 0.4992235600948334, steps = 9400, cost time = 1.21s
global_step/sec: 84.1326
loss = 0.45335766673088074, steps = 9500, cost time = 1.19s
global_step/sec: 83.4985
loss = 0.49295324087142944, steps = 9600, cost time = 1.20s
global_step/sec: 83.4257
loss = 0.48497188091278076, steps = 9700, cost time = 1.20s
global_step/sec: 83.4812
loss = 0.5185188055038452, steps = 9800, cost time = 1.20s
global_step/sec: 83.4058
loss = 0.46386241912841797, steps = 9900, cost time = 1.20s
global_step/sec: 83.5617
loss = 0.4530784785747528, steps = 10000, cost time = 1.20s
global_step/sec: 82.9686
loss = 0.4927741587162018, steps = 10100, cost time = 1.21s
global_step/sec: 83.5196
loss = 0.48471999168395996, steps = 10200, cost time = 1.20s
global_step/sec: 84.2398
loss = 0.46258217096328735, steps = 10300, cost time = 1.19s
global_step/sec: 83.9530
loss = 0.4734809398651123, steps = 10400, cost time = 1.19s
global_step/sec: 83.0677
loss = 0.5044661164283752, steps = 10500, cost time = 1.20s
global_step/sec: 84.6127
loss = 0.46837955713272095, steps = 10600, cost time = 1.18s
global_step/sec: 83.6754
loss = 0.5203244090080261, steps = 10700, cost time = 1.20s
global_step/sec: 84.3005
loss = 0.4886557459831238, steps = 10800, cost time = 1.19s
global_step/sec: 84.4909
loss = 0.4573897123336792, steps = 10900, cost time = 1.18s
global_step/sec: 83.5042
loss = 0.5009018182754517, steps = 11000, cost time = 1.20s
global_step/sec: 83.8476
loss = 0.4956497251987457, steps = 11100, cost time = 1.19s
global_step/sec: 83.7182
loss = 0.44263190031051636, steps = 11200, cost time = 1.19s
global_step/sec: 84.1451
loss = 0.48467421531677246, steps = 11300, cost time = 1.19s
global_step/sec: 83.6574
loss = 0.47025835514068604, steps = 11400, cost time = 1.20s
global_step/sec: 83.8682
loss = 0.47611501812934875, steps = 11500, cost time = 1.19s
global_step/sec: 83.5487
loss = 0.46327972412109375, steps = 11600, cost time = 1.20s
global_step/sec: 83.1680
loss = 0.49579286575317383, steps = 11700, cost time = 1.20s
global_step/sec: 83.4367
loss = 0.48085910081863403, steps = 11800, cost time = 1.20s
global_step/sec: 83.9930
loss = 0.5158828496932983, steps = 11900, cost time = 1.19s
global_step/sec: 83.5099
loss = 0.4685170650482178, steps = 12000, cost time = 1.20s
global_step/sec: 83.1610
loss = 0.48567521572113037, steps = 12100, cost time = 1.20s
global_step/sec: 83.1736
loss = 0.4538423418998718, steps = 12200, cost time = 1.20s
global_step/sec: 84.2121
loss = 0.490154504776001, steps = 12300, cost time = 1.19s
global_step/sec: 83.1291
loss = 0.4971981644630432, steps = 12400, cost time = 1.20s
global_step/sec: 83.8392
loss = 0.45952701568603516, steps = 12500, cost time = 1.19s
global_step/sec: 84.3396
loss = 0.42838045954704285, steps = 12600, cost time = 1.19s
global_step/sec: 82.7326
loss = 0.43322697281837463, steps = 12700, cost time = 1.21s
global_step/sec: 83.8764
loss = 0.48659539222717285, steps = 12800, cost time = 1.19s
global_step/sec: 82.9795
loss = 0.5176276564598083, steps = 12900, cost time = 1.21s
global_step/sec: 84.3671
loss = 0.5220683813095093, steps = 13000, cost time = 1.19s
global_step/sec: 84.3441
loss = 0.5057163238525391, steps = 13100, cost time = 1.19s
global_step/sec: 83.1759
loss = 0.5022393465042114, steps = 13200, cost time = 1.20s
global_step/sec: 83.2797
loss = 0.48838961124420166, steps = 13300, cost time = 1.20s
global_step/sec: 83.4463
loss = 0.5236831903457642, steps = 13400, cost time = 1.20s
global_step/sec: 82.7133
loss = 0.5133962035179138, steps = 13500, cost time = 1.21s
global_step/sec: 83.5593
loss = 0.5398536920547485, steps = 13600, cost time = 1.20s
global_step/sec: 83.8081
loss = 0.5025538802146912, steps = 13700, cost time = 1.19s
global_step/sec: 82.9005
loss = 0.4943029582500458, steps = 13800, cost time = 1.21s
global_step/sec: 84.0690
loss = 0.48722317814826965, steps = 13900, cost time = 1.19s
global_step/sec: 83.3496
loss = 0.514870285987854, steps = 14000, cost time = 1.20s
global_step/sec: 83.3161
loss = 0.4851018190383911, steps = 14100, cost time = 1.20s
global_step/sec: 83.9926
loss = 0.5167009830474854, steps = 14200, cost time = 1.19s
global_step/sec: 84.0176
loss = 0.4451643228530884, steps = 14300, cost time = 1.19s
global_step/sec: 83.6984
loss = 0.4888782203197479, steps = 14400, cost time = 1.19s
global_step/sec: 84.5286
loss = 0.48651760816574097, steps = 14500, cost time = 1.18s
global_step/sec: 83.5768
loss = 0.49005699157714844, steps = 14600, cost time = 1.20s
global_step/sec: 83.8816
loss = 0.46010833978652954, steps = 14700, cost time = 1.19s
global_step/sec: 83.2997
loss = 0.5019371509552002, steps = 14800, cost time = 1.20s
global_step/sec: 83.3525
loss = 0.48843806982040405, steps = 14900, cost time = 1.20s
global_step/sec: 83.3624
loss = 0.5050559043884277, steps = 15000, cost time = 1.20s
global_step/sec: 84.1202
loss = 0.4674615263938904, steps = 15100, cost time = 1.19s
global_step/sec: 84.0936
loss = 0.500991702079773, steps = 15200, cost time = 1.19s
global_step/sec: 83.5244
loss = 0.4944213628768921, steps = 15300, cost time = 1.20s
global_step/sec: 83.8108
loss = 0.45716604590415955, steps = 15400, cost time = 1.19s
global_step/sec: 84.1650
loss = 0.5364202857017517, steps = 15500, cost time = 1.19s
global_step/sec: 83.4699
loss = 0.44871553778648376, steps = 15600, cost time = 1.20s
global_step/sec: 82.8961
loss = 0.4143117070198059, steps = 15700, cost time = 1.21s
global_step/sec: 83.9989
loss = 0.4638308882713318, steps = 15800, cost time = 1.19s
global_step/sec: 84.3888
loss = 0.5165464282035828, steps = 15900, cost time = 1.18s
global_step/sec: 84.4660
loss = 0.4985188841819763, steps = 16000, cost time = 1.18s
global_step/sec: 82.9539
loss = 0.5260379910469055, steps = 16100, cost time = 1.21s
global_step/sec: 83.2071
loss = 0.5045164823532104, steps = 16200, cost time = 1.20s
global_step/sec: 83.4795
loss = 0.46578100323677063, steps = 16300, cost time = 1.20s
global_step/sec: 83.9925
loss = 0.5185701251029968, steps = 16400, cost time = 1.19s
global_step/sec: 84.5912
loss = 0.5173587799072266, steps = 16500, cost time = 1.18s
global_step/sec: 83.6776
loss = 0.4569825530052185, steps = 16600, cost time = 1.20s
global_step/sec: 83.6735
loss = 0.521152138710022, steps = 16700, cost time = 1.20s
global_step/sec: 83.9573
loss = 0.511492908000946, steps = 16800, cost time = 1.19s
global_step/sec: 84.2370
loss = 0.46419572830200195, steps = 16900, cost time = 1.19s
global_step/sec: 83.7531
loss = 0.4837857484817505, steps = 17000, cost time = 1.19s
global_step/sec: 83.8489
loss = 0.49949657917022705, steps = 17100, cost time = 1.19s
global_step/sec: 83.5531
loss = 0.46196049451828003, steps = 17200, cost time = 1.20s
global_step/sec: 83.6039
loss = 0.4947183132171631, steps = 17300, cost time = 1.20s
global_step/sec: 84.0988
loss = 0.485339492559433, steps = 17400, cost time = 1.19s
global_step/sec: 83.5035
loss = 0.4677164554595947, steps = 17500, cost time = 1.20s
global_step/sec: 82.2121
loss = 0.5021244287490845, steps = 17600, cost time = 1.22s
global_step/sec: 83.9801
loss = 0.5119506120681763, steps = 17700, cost time = 1.19s
global_step/sec: 84.8656
loss = 0.5019152164459229, steps = 17800, cost time = 1.18s
global_step/sec: 84.7424
loss = 0.5095879435539246, steps = 17900, cost time = 1.18s
global_step/sec: 83.6787
loss = 0.4725308418273926, steps = 18000, cost time = 1.20s
global_step/sec: 83.5610
loss = 0.4646216034889221, steps = 18100, cost time = 1.20s
global_step/sec: 83.6212
loss = 0.5099397897720337, steps = 18200, cost time = 1.20s
global_step/sec: 83.8583
loss = 0.48538994789123535, steps = 18300, cost time = 1.19s
global_step/sec: 83.6594
loss = 0.46903908252716064, steps = 18400, cost time = 1.20s
global_step/sec: 84.0182
loss = 0.4954080581665039, steps = 18500, cost time = 1.19s
global_step/sec: 84.0542
loss = 0.5245227813720703, steps = 18600, cost time = 1.19s
global_step/sec: 83.0572
loss = 0.5109649896621704, steps = 18700, cost time = 1.20s
global_step/sec: 82.8340
loss = 0.4905576705932617, steps = 18800, cost time = 1.21s
global_step/sec: 85.0861
loss = 0.5086418390274048, steps = 18900, cost time = 1.18s
global_step/sec: 83.6165
loss = 0.476216197013855, steps = 19000, cost time = 1.20s
global_step/sec: 83.5232