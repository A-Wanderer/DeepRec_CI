INFO:tensorflow:Parsing ./data/train.csv
INFO:tensorflow:Parsing ./data/eval.csv
INFO:tensorflow:Is using fused embedding lookup for this scope C10_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope C11_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope C12_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope C13_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope C14_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope C15_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope C16_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope C17_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope C18_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope C19_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope C1_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope C20_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope C21_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope C22_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope C23_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope C24_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope C25_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope C26_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope C2_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope C3_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope C4_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope C5_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope C6_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope C7_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope C8_embedding_weights
INFO:tensorflow:Is using fused embedding lookup for this scope C9_embedding_weights
2022-03-28 05:27:20.148741: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3400000000 Hz
2022-03-28 05:27:20.151110: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3f63140 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-03-28 05:27:20.151134: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
Using TensorFlow version 1.15.5
Checking dataset
Numbers of training dataset is 8000000
Numbers of test dataset is 2000000
Saving model checkpoints to /benchmark_result/checkpoint/2022-03-28-13-27-11/dlrm_deeprec_fp32_--fusion
global_step/sec: 42.7743
loss = 1.0398818254470825, steps = 0, cost time = 2.34s
global_step/sec: 102.3635
loss = 0.47473517060279846, steps = 100, cost time = 0.98s
global_step/sec: 99.6448
loss = 0.4521939158439636, steps = 200, cost time = 1.00s
global_step/sec: 115.2733
loss = 0.5295175313949585, steps = 300, cost time = 0.87s
global_step/sec: 115.8771
loss = 0.526672899723053, steps = 400, cost time = 0.86s
global_step/sec: 116.1852
loss = 0.5013993978500366, steps = 500, cost time = 0.86s
global_step/sec: 115.3501
loss = 0.5552017688751221, steps = 600, cost time = 0.87s
global_step/sec: 115.3750
loss = 0.5622878074645996, steps = 700, cost time = 0.87s
global_step/sec: 115.6324
loss = 0.5255340337753296, steps = 800, cost time = 0.86s
global_step/sec: 115.5908
loss = 0.5056904554367065, steps = 900, cost time = 0.87s
global_step/sec: 116.6488
loss = 0.5293915271759033, steps = 1000, cost time = 0.86s
global_step/sec: 116.0921
loss = 0.5314858555793762, steps = 1100, cost time = 0.86s
global_step/sec: 114.9593
loss = 0.5012226700782776, steps = 1200, cost time = 0.87s
global_step/sec: 115.9902
loss = 0.4889812469482422, steps = 1300, cost time = 0.86s
global_step/sec: 115.2749
loss = 0.48935431241989136, steps = 1400, cost time = 0.87s
global_step/sec: 116.1594
loss = 0.47781267762184143, steps = 1500, cost time = 0.86s
global_step/sec: 116.4438
loss = 0.4754064083099365, steps = 1600, cost time = 0.86s
global_step/sec: 114.1215
loss = 0.522477388381958, steps = 1700, cost time = 0.88s
global_step/sec: 115.4914
loss = 0.48932310938835144, steps = 1800, cost time = 0.87s
global_step/sec: 115.7465
loss = 0.49413245916366577, steps = 1900, cost time = 0.86s
global_step/sec: 115.9001
loss = 0.4690248370170593, steps = 2000, cost time = 0.86s
global_step/sec: 115.4941
loss = 0.47596806287765503, steps = 2100, cost time = 0.87s
global_step/sec: 115.5281
loss = 0.5315841436386108, steps = 2200, cost time = 0.87s
global_step/sec: 115.6727
loss = 0.4903927743434906, steps = 2300, cost time = 0.86s
global_step/sec: 115.0610
loss = 0.5073000192642212, steps = 2400, cost time = 0.87s
global_step/sec: 114.7031
loss = 0.46678799390792847, steps = 2500, cost time = 0.87s
global_step/sec: 116.1303
loss = 0.5041688084602356, steps = 2600, cost time = 0.86s
global_step/sec: 115.3657
loss = 0.5384661555290222, steps = 2700, cost time = 0.87s
global_step/sec: 115.9449
loss = 0.5027254223823547, steps = 2800, cost time = 0.86s
global_step/sec: 116.0145
loss = 0.5088648796081543, steps = 2900, cost time = 0.86s
global_step/sec: 116.3743
loss = 0.5134991407394409, steps = 3000, cost time = 0.86s
global_step/sec: 115.9425
loss = 0.5042125582695007, steps = 3100, cost time = 0.86s
global_step/sec: 115.4415
loss = 0.4926197826862335, steps = 3200, cost time = 0.87s
global_step/sec: 116.1340
loss = 0.49181365966796875, steps = 3300, cost time = 0.86s
global_step/sec: 115.7465
loss = 0.4819757342338562, steps = 3400, cost time = 0.86s
global_step/sec: 115.1267
loss = 0.5010508298873901, steps = 3500, cost time = 0.87s
global_step/sec: 114.7876
loss = 0.466111958026886, steps = 3600, cost time = 0.87s
global_step/sec: 115.4203
loss = 0.47963613271713257, steps = 3700, cost time = 0.87s
global_step/sec: 115.2187
loss = 0.4927087426185608, steps = 3800, cost time = 0.87s
global_step/sec: 114.7705
loss = 0.5002862215042114, steps = 3900, cost time = 0.87s
global_step/sec: 114.9492
loss = 0.5334373712539673, steps = 4000, cost time = 0.87s
global_step/sec: 114.8887
loss = 0.4860314726829529, steps = 4100, cost time = 0.87s
global_step/sec: 114.9603
loss = 0.454276978969574, steps = 4200, cost time = 0.87s
global_step/sec: 115.2205
loss = 0.5126467943191528, steps = 4300, cost time = 0.87s
global_step/sec: 115.8871
loss = 0.48403024673461914, steps = 4400, cost time = 0.86s
global_step/sec: 114.5820
loss = 0.4924086928367615, steps = 4500, cost time = 0.87s
global_step/sec: 114.8144
loss = 0.48501908779144287, steps = 4600, cost time = 0.87s
global_step/sec: 115.0254
loss = 0.4534224271774292, steps = 4700, cost time = 0.87s
global_step/sec: 115.9915
loss = 0.49597835540771484, steps = 4800, cost time = 0.86s
global_step/sec: 115.4085
loss = 0.47263103723526, steps = 4900, cost time = 0.87s
global_step/sec: 115.5556
loss = 0.47905853390693665, steps = 5000, cost time = 0.87s
global_step/sec: 114.9983
loss = 0.4885611832141876, steps = 5100, cost time = 0.87s
global_step/sec: 115.6419
loss = 0.4996355175971985, steps = 5200, cost time = 0.86s
global_step/sec: 115.1824
loss = 0.4705408811569214, steps = 5300, cost time = 0.87s
global_step/sec: 115.6893
loss = 0.5180664658546448, steps = 5400, cost time = 0.86s
global_step/sec: 114.7965
loss = 0.49882790446281433, steps = 5500, cost time = 0.87s
global_step/sec: 114.5848
loss = 0.46454519033432007, steps = 5600, cost time = 0.87s
global_step/sec: 116.1215
loss = 0.5005003809928894, steps = 5700, cost time = 0.86s
global_step/sec: 115.2268
loss = 0.5086548328399658, steps = 5800, cost time = 0.87s
global_step/sec: 114.5975
loss = 0.5036013126373291, steps = 5900, cost time = 0.87s
global_step/sec: 114.3877
loss = 0.4844810962677002, steps = 6000, cost time = 0.87s
global_step/sec: 115.3712
loss = 0.5138142704963684, steps = 6100, cost time = 0.87s
global_step/sec: 115.3439
loss = 0.479892373085022, steps = 6200, cost time = 0.87s
global_step/sec: 115.5991
loss = 0.5208936929702759, steps = 6300, cost time = 0.87s
global_step/sec: 116.4472
loss = 0.5171003937721252, steps = 6400, cost time = 0.86s
global_step/sec: 114.7150
loss = 0.46390241384506226, steps = 6500, cost time = 0.87s
global_step/sec: 115.8193
loss = 0.47970056533813477, steps = 6600, cost time = 0.86s
global_step/sec: 115.7333
loss = 0.49466490745544434, steps = 6700, cost time = 0.86s
global_step/sec: 116.0480
loss = 0.4739527106285095, steps = 6800, cost time = 0.86s
global_step/sec: 115.1579
loss = 0.4476618766784668, steps = 6900, cost time = 0.87s
global_step/sec: 116.0003
loss = 0.5062040686607361, steps = 7000, cost time = 0.86s
global_step/sec: 115.4085
loss = 0.5408825874328613, steps = 7100, cost time = 0.87s
global_step/sec: 115.3207
loss = 0.4635666012763977, steps = 7200, cost time = 0.87s
global_step/sec: 115.4764
loss = 0.4869745969772339, steps = 7300, cost time = 0.87s
global_step/sec: 115.2794
loss = 0.49684298038482666, steps = 7400, cost time = 0.87s
global_step/sec: 115.7566
loss = 0.48176831007003784, steps = 7500, cost time = 0.86s
global_step/sec: 116.1450
loss = 0.4910914897918701, steps = 7600, cost time = 0.86s
global_step/sec: 115.4785
loss = 0.4239926040172577, steps = 7700, cost time = 0.87s
global_step/sec: 115.2320
loss = 0.42368578910827637, steps = 7800, cost time = 0.87s
global_step/sec: 114.7322
loss = 0.4826887249946594, steps = 7900, cost time = 0.87s
global_step/sec: 116.3421
loss = 0.4538816809654236, steps = 8000, cost time = 0.86s
global_step/sec: 115.3534
loss = 0.4759708642959595, steps = 8100, cost time = 0.87s
global_step/sec: 114.5585
loss = 0.48528003692626953, steps = 8200, cost time = 0.87s
global_step/sec: 115.6530
loss = 0.4499472379684448, steps = 8300, cost time = 0.86s
global_step/sec: 115.5034
loss = 0.482354074716568, steps = 8400, cost time = 0.87s
global_step/sec: 115.2627
loss = 0.49610304832458496, steps = 8500, cost time = 0.87s
global_step/sec: 114.9451
loss = 0.4715323746204376, steps = 8600, cost time = 0.87s
global_step/sec: 114.9344
loss = 0.47488653659820557, steps = 8700, cost time = 0.87s
global_step/sec: 115.3613
loss = 0.48251116275787354, steps = 8800, cost time = 0.87s
global_step/sec: 115.7734
loss = 0.46740779280662537, steps = 8900, cost time = 0.86s
global_step/sec: 115.5367
loss = 0.4886433482170105, steps = 9000, cost time = 0.87s
global_step/sec: 114.9079
loss = 0.5066927075386047, steps = 9100, cost time = 0.87s
global_step/sec: 115.8065
loss = 0.47885245084762573, steps = 9200, cost time = 0.86s
global_step/sec: 114.4735
loss = 0.5035159587860107, steps = 9300, cost time = 0.87s
global_step/sec: 115.9538
loss = 0.5045488476753235, steps = 9400, cost time = 0.86s
global_step/sec: 115.5854
loss = 0.44585251808166504, steps = 9500, cost time = 0.87s
global_step/sec: 116.5844
loss = 0.49007999897003174, steps = 9600, cost time = 0.86s
global_step/sec: 115.8094
loss = 0.47683584690093994, steps = 9700, cost time = 0.86s
global_step/sec: 116.1127
loss = 0.5220359563827515, steps = 9800, cost time = 0.86s
global_step/sec: 115.5115
loss = 0.4583415985107422, steps = 9900, cost time = 0.87s
global_step/sec: 115.4078
loss = 0.4512641429901123, steps = 10000, cost time = 0.87s
global_step/sec: 115.4131
loss = 0.4873315393924713, steps = 10100, cost time = 0.87s
global_step/sec: 114.4871
loss = 0.4723666310310364, steps = 10200, cost time = 0.87s
global_step/sec: 115.3881
loss = 0.468750923871994, steps = 10300, cost time = 0.87s
global_step/sec: 115.2076
loss = 0.4683295786380768, steps = 10400, cost time = 0.87s
global_step/sec: 116.4684
loss = 0.5024359226226807, steps = 10500, cost time = 0.86s
global_step/sec: 115.1545
loss = 0.4584220051765442, steps = 10600, cost time = 0.87s
global_step/sec: 114.8886
loss = 0.5126703977584839, steps = 10700, cost time = 0.87s
global_step/sec: 115.7940
loss = 0.47351568937301636, steps = 10800, cost time = 0.86s
global_step/sec: 115.6276
loss = 0.4556705355644226, steps = 10900, cost time = 0.86s
global_step/sec: 115.2216
loss = 0.4885409474372864, steps = 11000, cost time = 0.87s
global_step/sec: 115.2720
loss = 0.49561429023742676, steps = 11100, cost time = 0.87s
global_step/sec: 116.0272
loss = 0.441251277923584, steps = 11200, cost time = 0.86s
global_step/sec: 115.4130
loss = 0.4768572449684143, steps = 11300, cost time = 0.87s
global_step/sec: 114.9228
loss = 0.4665903151035309, steps = 11400, cost time = 0.87s
global_step/sec: 115.0277
loss = 0.47027480602264404, steps = 11500, cost time = 0.87s
global_step/sec: 116.4202
loss = 0.4624090790748596, steps = 11600, cost time = 0.86s
global_step/sec: 114.9784
loss = 0.4777272939682007, steps = 11700, cost time = 0.87s
global_step/sec: 115.8173
loss = 0.4798506498336792, steps = 11800, cost time = 0.86s
global_step/sec: 115.0322
loss = 0.5005769729614258, steps = 11900, cost time = 0.87s
global_step/sec: 114.7864
loss = 0.4660986363887787, steps = 12000, cost time = 0.87s
global_step/sec: 115.4283
loss = 0.4896373152732849, steps = 12100, cost time = 0.87s
global_step/sec: 116.2647
loss = 0.4464716911315918, steps = 12200, cost time = 0.86s
global_step/sec: 115.0904
loss = 0.49912625551223755, steps = 12300, cost time = 0.87s
global_step/sec: 115.6805
loss = 0.49632734060287476, steps = 12400, cost time = 0.86s
global_step/sec: 114.7187
loss = 0.4507046639919281, steps = 12500, cost time = 0.87s
global_step/sec: 115.8310
loss = 0.4278261065483093, steps = 12600, cost time = 0.86s
global_step/sec: 114.9371
loss = 0.43794065713882446, steps = 12700, cost time = 0.87s
global_step/sec: 115.0603
loss = 0.48628532886505127, steps = 12800, cost time = 0.87s
global_step/sec: 114.6437
loss = 0.5278642177581787, steps = 12900, cost time = 0.87s
global_step/sec: 114.6172
loss = 0.5300837755203247, steps = 13000, cost time = 0.87s
global_step/sec: 115.2627
loss = 0.5143352746963501, steps = 13100, cost time = 0.87s
global_step/sec: 115.7010
loss = 0.49604925513267517, steps = 13200, cost time = 0.86s
global_step/sec: 115.4488
loss = 0.502979040145874, steps = 13300, cost time = 0.87s
global_step/sec: 115.6712
loss = 0.5090669989585876, steps = 13400, cost time = 0.86s
global_step/sec: 116.8104
loss = 0.5094074606895447, steps = 13500, cost time = 0.86s
global_step/sec: 115.6514
loss = 0.5353499054908752, steps = 13600, cost time = 0.86s
global_step/sec: 115.5879
loss = 0.5057874917984009, steps = 13700, cost time = 0.87s
global_step/sec: 116.2329
loss = 0.49346160888671875, steps = 13800, cost time = 0.86s
global_step/sec: 115.8590
loss = 0.49102675914764404, steps = 13900, cost time = 0.86s
global_step/sec: 115.3254
loss = 0.5170893669128418, steps = 14000, cost time = 0.87s
global_step/sec: 115.2197
loss = 0.4993457794189453, steps = 14100, cost time = 0.87s
global_step/sec: 115.6473
loss = 0.4930540919303894, steps = 14200, cost time = 0.86s
global_step/sec: 116.2029
loss = 0.4484010338783264, steps = 14300, cost time = 0.86s
global_step/sec: 115.7411
loss = 0.478115975856781, steps = 14400, cost time = 0.86s
global_step/sec: 115.8665
loss = 0.48291224241256714, steps = 14500, cost time = 0.86s
global_step/sec: 115.6997
loss = 0.4943254292011261, steps = 14600, cost time = 0.86s
global_step/sec: 115.7672
loss = 0.4547193646430969, steps = 14700, cost time = 0.86s
global_step/sec: 115.5433
loss = 0.49658945202827454, steps = 14800, cost time = 0.87s
global_step/sec: 115.5056
loss = 0.4805821180343628, steps = 14900, cost time = 0.87s
global_step/sec: 116.1616
loss = 0.5043150186538696, steps = 15000, cost time = 0.86s
global_step/sec: 115.5833
loss = 0.4545077085494995, steps = 15100, cost time = 0.87s
global_step/sec: 115.1454
loss = 0.4995276927947998, steps = 15200, cost time = 0.87s
global_step/sec: 115.5580
loss = 0.49729123711586, steps = 15300, cost time = 0.87s
global_step/sec: 115.9247
loss = 0.4443312883377075, steps = 15400, cost time = 0.86s
global_step/sec: 115.1794
loss = 0.5337641835212708, steps = 15500, cost time = 0.87s
global_step/sec: 116.3414
loss = 0.437279611825943, steps = 15600, cost time = 0.86s
global_step/sec: 115.3722
loss = 0.3987932801246643, steps = 15700, cost time = 0.87s
global_step/sec: 114.9933
loss = 0.4542938470840454, steps = 15800, cost time = 0.87s
global_step/sec: 114.7072
loss = 0.5091372132301331, steps = 15900, cost time = 0.87s
global_step/sec: 116.0375
loss = 0.48436394333839417, steps = 16000, cost time = 0.86s
global_step/sec: 115.8192
loss = 0.5240452289581299, steps = 16100, cost time = 0.86s
global_step/sec: 115.4193
loss = 0.5064631700515747, steps = 16200, cost time = 0.87s
global_step/sec: 115.4157
loss = 0.4598621129989624, steps = 16300, cost time = 0.87s
global_step/sec: 115.6110
loss = 0.5163279175758362, steps = 16400, cost time = 0.86s
global_step/sec: 116.0181
loss = 0.5387153029441833, steps = 16500, cost time = 0.86s
global_step/sec: 114.8673
loss = 0.4645712077617645, steps = 16600, cost time = 0.87s
global_step/sec: 115.7651
loss = 0.5189691781997681, steps = 16700, cost time = 0.86s
global_step/sec: 115.3461
loss = 0.5080044865608215, steps = 16800, cost time = 0.87s
global_step/sec: 115.3114
loss = 0.45677608251571655, steps = 16900, cost time = 0.87s
global_step/sec: 115.7167
loss = 0.4869154095649719, steps = 17000, cost time = 0.86s
global_step/sec: 115.7554
loss = 0.4926798343658447, steps = 17100, cost time = 0.86s
global_step/sec: 115.2852
loss = 0.4489448070526123, steps = 17200, cost time = 0.87s
global_step/sec: 115.1173
loss = 0.48542433977127075, steps = 17300, cost time = 0.87s
global_step/sec: 115.2984
loss = 0.4758872091770172, steps = 17400, cost time = 0.87s
global_step/sec: 116.5053
loss = 0.4600849747657776, steps = 17500, cost time = 0.86s
global_step/sec: 115.5067
loss = 0.5106916427612305, steps = 17600, cost time = 0.87s
global_step/sec: 115.5487
loss = 0.49468404054641724, steps = 17700, cost time = 0.87s
global_step/sec: 115.0615
loss = 0.4949215352535248, steps = 17800, cost time = 0.87s
global_step/sec: 115.8702
loss = 0.5077370405197144, steps = 17900, cost time = 0.86s
global_step/sec: 114.3903
loss = 0.45659568905830383, steps = 18000, cost time = 0.87s
global_step/sec: 115.5044
loss = 0.47103390097618103, steps = 18100, cost time = 0.87s
global_step/sec: 114.7842
loss = 0.49593764543533325, steps = 18200, cost time = 0.87s
global_step/sec: 116.2714
loss = 0.47655782103538513, steps = 18300, cost time = 0.86s
global_step/sec: 116.0047
loss = 0.47705358266830444, steps = 18400, cost time = 0.86s
global_step/sec: 114.9261
loss = 0.4948641359806061, steps = 18500, cost time = 0.87s
global_step/sec: 116.8760
loss = 0.5196973085403442, steps = 18600, cost time = 0.86s
global_step/sec: 115.3252
loss = 0.5156726837158203, steps = 18700, cost time = 0.87s
global_step/sec: 115.6896
loss = 0.4857714772224426, steps = 18800, cost time = 0.86s
global_step/sec: 116.0890
loss = 0.5020447969436646, steps = 18900, cost time = 0.86s
global_step/sec: 115.9393
loss = 0.4761805534362793, steps = 19000, cost time = 0.86s
global_step/sec: 115.4556
loss = 0.47544223070144653, steps = 19100, cost time = 0.87s
global_step/sec: 115.8582
loss = 0.4651915431022644, steps = 19200, cost time = 0.86s
global_step/sec: 116.3856
loss = 0.49243348836898804, steps = 19300, cost time = 0.86s
global_step/sec: 114.8554
loss = 0.4601852595806122, steps = 19400, cost time = 0.87s
global_step/sec: 115.9442
loss = 0.496278315782547, steps = 19500, cost time = 0.86s
global_step/sec: 116.0353
loss = 0.5022543668746948, steps = 19600, cost time = 0.86s
global_step/sec: 115.1478
loss = 0.47083932161331177, steps = 19700, cost time = 0.87s
global_step/sec: 115.0491
loss = 0.4426894187927246, steps = 19800, cost time = 0.87s
global_step/sec: 115.3170
loss = 0.4308761954307556, steps = 19900, cost time = 0.87s
global_step/sec: 115.3384
loss = 0.48298168182373047, steps = 20000, cost time = 0.87s
global_step/sec: 115.1691
loss = 0.47836822271347046, steps = 20100, cost time = 0.87s
global_step/sec: 115.3467
loss = 0.5157920122146606, steps = 20200, cost time = 0.87s
global_step/sec: 115.1018
loss = 0.45490267872810364, steps = 20300, cost time = 0.87s
global_step/sec: 116.4488
loss = 0.48815757036209106, steps = 20400, cost time = 0.86s
global_step/sec: 115.5464
loss = 0.4755440652370453, steps = 20500, cost time = 0.87s
global_step/sec: 115.5675
loss = 0.45945945382118225, steps = 20600, cost time = 0.87s
global_step/sec: 115.8270
loss = 0.479311466217041, steps = 20700, cost time = 0.86s
global_step/sec: 115.7117
loss = 0.4783139228820801, steps = 20800, cost time = 0.86s
global_step/sec: 114.7952
loss = 0.5120970010757446, steps = 20900, cost time = 0.87s
global_step/sec: 115.1745
loss = 0.49659332633018494, steps = 21000, cost time = 0.87s
global_step/sec: 115.5897
loss = 0.45203301310539246, steps = 21100, cost time = 0.87s
global_step/sec: 113.8426
loss = 0.4993281364440918, steps = 21200, cost time = 0.88s
global_step/sec: 115.8566
loss = 0.4552583694458008, steps = 21300, cost time = 0.86s
global_step/sec: 115.4813
loss = 0.4513508975505829, steps = 21400, cost time = 0.87s
global_step/sec: 115.5998
loss = 0.4755503535270691, steps = 21500, cost time = 0.87s
global_step/sec: 115.6404
loss = 0.49423301219940186, steps = 21600, cost time = 0.86s
global_step/sec: 116.5334
loss = 0.5021586418151855, steps = 21700, cost time = 0.86s
global_step/sec: 115.3289
loss = 0.47421586513519287, steps = 21800, cost time = 0.87s
global_step/sec: 115.7061
loss = 0.47656309604644775, steps = 21900, cost time = 0.86s
global_step/sec: 114.9519
loss = 0.49964484572410583, steps = 22000, cost time = 0.87s
global_step/sec: 116.3714
loss = 0.5013152956962585, steps = 22100, cost time = 0.86s
global_step/sec: 115.6471
loss = 0.4807048439979553, steps = 22200, cost time = 0.86s
global_step/sec: 115.9515
loss = 0.45660048723220825, steps = 22300, cost time = 0.86s
global_step/sec: 115.2993
loss = 0.5079913139343262, steps = 22400, cost time = 0.87s
global_step/sec: 115.0765
loss = 0.46304377913475037, steps = 22500, cost time = 0.87s
global_step/sec: 115.4781
loss = 0.48849281668663025, steps = 22600, cost time = 0.87s
global_step/sec: 115.5178
loss = 0.4412199556827545, steps = 22700, cost time = 0.87s
global_step/sec: 114.9538
loss = 0.44315457344055176, steps = 22800, cost time = 0.87s
global_step/sec: 115.1438
loss = 0.48501208424568176, steps = 22900, cost time = 0.87s
global_step/sec: 115.9645
loss = 0.48224714398384094, steps = 23000, cost time = 0.86s
global_step/sec: 115.3536
loss = 0.488882839679718, steps = 23100, cost time = 0.87s
global_step/sec: 115.5109
loss = 0.47998473048210144, steps = 23200, cost time = 0.87s
global_step/sec: 114.8571
loss = 0.48042261600494385, steps = 23300, cost time = 0.87s
global_step/sec: 115.3103
loss = 0.4755939841270447, steps = 23400, cost time = 0.87s
global_step/sec: 115.4062
loss = 0.46134692430496216, steps = 23500, cost time = 0.87s
global_step/sec: 115.6258
loss = 0.5020692348480225, steps = 23600, cost time = 0.86s
global_step/sec: 115.5950
loss = 0.45608994364738464, steps = 23700, cost time = 0.87s
global_step/sec: 115.2541
loss = 0.46634048223495483, steps = 23800, cost time = 0.87s
global_step/sec: 115.5527
loss = 0.4422825574874878, steps = 23900, cost time = 0.87s
global_step/sec: 115.1533
loss = 0.4485113322734833, steps = 24000, cost time = 0.87s
global_step/sec: 115.3303
loss = 0.4385351538658142, steps = 24100, cost time = 0.87s
global_step/sec: 116.0834
loss = 0.43779057264328003, steps = 24200, cost time = 0.86s
global_step/sec: 116.3265
loss = 0.4900226294994354, steps = 24300, cost time = 0.86s
global_step/sec: 115.3118
loss = 0.4711076617240906, steps = 24400, cost time = 0.87s
global_step/sec: 114.8924
loss = 0.4605304002761841, steps = 24500, cost time = 0.87s
global_step/sec: 114.5496
loss = 0.48457643389701843, steps = 24600, cost time = 0.87s
global_step/sec: 114.9070
loss = 0.4261385202407837, steps = 24700, cost time = 0.87s
global_step/sec: 114.8287
loss = 0.48711276054382324, steps = 24800, cost time = 0.87s
global_step/sec: 116.0392
loss = 0.4520127773284912, steps = 24900, cost time = 0.86s
global_step/sec: 115.6041
loss = 0.4815497398376465, steps = 25000, cost time = 0.87s
global_step/sec: 115.6068
loss = 0.46680212020874023, steps = 25100, cost time = 0.87s
global_step/sec: 115.4763
loss = 0.48099231719970703, steps = 25200, cost time = 0.87s
global_step/sec: 115.5214
loss = 0.44181951880455017, steps = 25300, cost time = 0.87s
global_step/sec: 115.2134
loss = 0.42885515093803406, steps = 25400, cost time = 0.87s
global_step/sec: 114.5022
loss = 0.48918503522872925, steps = 25500, cost time = 0.87s
global_step/sec: 115.4120
loss = 0.4757048487663269, steps = 25600, cost time = 0.87s
global_step/sec: 115.7175
loss = 0.4447590112686157, steps = 25700, cost time = 0.86s
global_step/sec: 115.1468
loss = 0.45395922660827637, steps = 25800, cost time = 0.87s
global_step/sec: 115.0738
loss = 0.4769992232322693, steps = 25900, cost time = 0.87s
global_step/sec: 115.1590
loss = 0.4942922592163086, steps = 26000, cost time = 0.87s
global_step/sec: 115.0051
loss = 0.45390963554382324, steps = 26100, cost time = 0.87s
global_step/sec: 115.3003
loss = 0.4277932047843933, steps = 26200, cost time = 0.87s
global_step/sec: 115.3040
loss = 0.46523499488830566, steps = 26300, cost time = 0.87s
global_step/sec: 116.0082
loss = 0.4578697085380554, steps = 26400, cost time = 0.86s
global_step/sec: 115.6496
loss = 0.46747472882270813, steps = 26500, cost time = 0.86s
global_step/sec: 115.6543
loss = 0.4635681211948395, steps = 26600, cost time = 0.86s
global_step/sec: 115.4977
loss = 0.44843190908432007, steps = 26700, cost time = 0.87s
global_step/sec: 115.9152
loss = 0.48897138237953186, steps = 26800, cost time = 0.86s
global_step/sec: 115.3989
loss = 0.47209304571151733, steps = 26900, cost time = 0.87s
global_step/sec: 114.9071
loss = 0.49758416414260864, steps = 27000, cost time = 0.87s
global_step/sec: 115.0933
loss = 0.46968191862106323, steps = 27100, cost time = 0.87s
global_step/sec: 115.1157
loss = 0.46560660004615784, steps = 27200, cost time = 0.87s
global_step/sec: 115.6183
loss = 0.4828524589538574, steps = 27300, cost time = 0.86s
global_step/sec: 114.6269
loss = 0.4505869448184967, steps = 27400, cost time = 0.87s
global_step/sec: 114.7804
loss = 0.46429669857025146, steps = 27500, cost time = 0.87s
global_step/sec: 115.9814
loss = 0.42240142822265625, steps = 27600, cost time = 0.86s
global_step/sec: 115.0469
loss = 0.508160412311554, steps = 27700, cost time = 0.87s
global_step/sec: 115.7247
loss = 0.4634828567504883, steps = 27800, cost time = 0.86s
global_step/sec: 114.7476
loss = 0.4764307141304016, steps = 27900, cost time = 0.87s
global_step/sec: 115.0830
loss = 0.5402430295944214, steps = 28000, cost time = 0.87s
global_step/sec: 116.0328
loss = 0.4704285264015198, steps = 28100, cost time = 0.86s
global_step/sec: 116.2270
loss = 0.45320791006088257, steps = 28200, cost time = 0.86s
global_step/sec: 114.7921
loss = 0.4519534409046173, steps = 28300, cost time = 0.87s
global_step/sec: 115.5469