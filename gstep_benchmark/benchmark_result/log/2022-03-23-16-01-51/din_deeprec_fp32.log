INFO:tensorflow:vocabulary_size = 543060 in UID is inferred from the number of elements in the vocabulary_file ./data/uid_voc.txt.
INFO:tensorflow:vocabulary_size = 367983 in ITEM is inferred from the number of elements in the vocabulary_file ./data/mid_voc.txt.
INFO:tensorflow:vocabulary_size = 1601 in CATEGORY is inferred from the number of elements in the vocabulary_file ./data/cat_voc.txt.
INFO:tensorflow:vocabulary_size = 367983 in HISTORY_ITEM is inferred from the number of elements in the vocabulary_file ./data/mid_voc.txt.
INFO:tensorflow:vocabulary_size = 1601 in HISTORY_CATEGORY is inferred from the number of elements in the vocabulary_file ./data/cat_voc.txt.
INFO:tensorflow:Parsing ./data/local_train_splitByUser
INFO:tensorflow:Parsing ./data/local_test_splitByUser
2022-03-23 08:10:41.542772: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3299995000 Hz
2022-03-23 08:10:41.548684: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f8eb95dfc00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-03-23 08:10:41.548726: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-03-23 08:10:44.089828: I tensorflow/core/profiler/lib/profiler_session.cc:205] Profiler session started.
2022-03-23 08:10:53.823936: I tensorflow/core/profiler/lib/profiler_session.cc:205] Profiler session started.
2022-03-23 08:11:02.316804: I tensorflow/core/profiler/lib/profiler_session.cc:205] Profiler session started.
Using TensorFlow version 1.15.5
Checking dataset
Numbers of training dataset is 1086120
Numbers of test dataset is 121216
Saving model checkpoints to /benchmark_result/checkpoint/2022-03-23-16-01-51/din_deeprec_fp32
Save timeline to /benchmark_result/checkpoint/2022-03-23-16-01-51/din_deeprec_fp32
global_step/sec: 38.6914
loss = 0.6882541179656982, steps = 0, cost time = 2.58s
global_step/sec: 111.6402
loss = 0.7003988027572632, steps = 100, cost time = 0.90s
global_step/sec: 98.0136
loss = 0.6883276700973511, steps = 200, cost time = 1.02s
global_step/sec: 118.7655
loss = 0.6625697612762451, steps = 300, cost time = 0.84s
global_step/sec: 118.1559
loss = 0.6625741720199585, steps = 400, cost time = 0.85s
global_step/sec: 118.6024
loss = 0.6672564744949341, steps = 500, cost time = 0.84s
global_step/sec: 118.1200
loss = 0.661007821559906, steps = 600, cost time = 0.85s
global_step/sec: 118.9349
loss = 0.6511771082878113, steps = 700, cost time = 0.84s
global_step/sec: 119.2059
loss = 0.6276467442512512, steps = 800, cost time = 0.84s
global_step/sec: 119.4361
loss = 0.6553451418876648, steps = 900, cost time = 0.84s
Save timeline to /benchmark_result/checkpoint/2022-03-23-16-01-51/din_deeprec_fp32
global_step/sec: 104.6324
loss = 0.6648658514022827, steps = 1000, cost time = 0.96s
global_step/sec: 119.8541
loss = 0.674894392490387, steps = 1100, cost time = 0.83s
global_step/sec: 119.4574
loss = 0.6425173282623291, steps = 1200, cost time = 0.84s
global_step/sec: 118.9881
loss = 0.596885621547699, steps = 1300, cost time = 0.84s
global_step/sec: 119.3499
loss = 0.6559533476829529, steps = 1400, cost time = 0.84s
global_step/sec: 119.0804
loss = 0.6623263359069824, steps = 1500, cost time = 0.84s
global_step/sec: 119.3614
loss = 0.6476041078567505, steps = 1600, cost time = 0.84s
global_step/sec: 119.9082
loss = 0.6372036933898926, steps = 1700, cost time = 0.83s
global_step/sec: 119.4383
loss = 0.6318333745002747, steps = 1800, cost time = 0.84s
global_step/sec: 118.9992
loss = 0.6496185064315796, steps = 1900, cost time = 0.84s
Save timeline to /benchmark_result/checkpoint/2022-03-23-16-01-51/din_deeprec_fp32
global_step/sec: 106.7167
loss = 0.6999220252037048, steps = 2000, cost time = 0.94s
global_step/sec: 118.6273
loss = 0.645704448223114, steps = 2100, cost time = 0.84s
global_step/sec: 119.2849
loss = 0.5861806869506836, steps = 2200, cost time = 0.84s
global_step/sec: 119.5713
loss = 0.6266822814941406, steps = 2300, cost time = 0.84s
global_step/sec: 119.6013
loss = 0.6165604591369629, steps = 2400, cost time = 0.84s
global_step/sec: 119.0338
loss = 0.659782886505127, steps = 2500, cost time = 0.84s
global_step/sec: 118.8487
loss = 0.5788506269454956, steps = 2600, cost time = 0.84s
global_step/sec: 119.5507
loss = 0.6666927337646484, steps = 2700, cost time = 0.84s
global_step/sec: 118.6522
loss = 0.5967523455619812, steps = 2800, cost time = 0.84s
global_step/sec: 118.6532
loss = 0.5843110084533691, steps = 2900, cost time = 0.84s
global_step/sec: 3549.3568
loss = 0.6029961109161377, steps = 2999, cost time = 0.84s
